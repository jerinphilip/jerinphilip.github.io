<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Jerin Philip</title>
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Sampling from Distributions</h1>
                        <div class="row">
    <div class="toc col-md-4 col-sm-12 push-md-8"><h6>Outline</h6><ul>
<li><a href="#premise">Premise</a></li>
<li><a href="#lets-sample">Let’s sample</a><ul>
<li><a href="#inverse-transform-sampling">Inverse Transform Sampling</a></li>
<li><a href="#verification">Verification</a></li>
</ul></li>
<li><a href="#karpathys-char-rnn">Karpathy’s Char RNN</a><ul>
<li><a href="#temperature">Temperature</a></li>
</ul></li>
</ul></div><div class="col-md-8 col-sm-12 post-content"><h1 id="premise">Premise</h1>
<p>The objective we start with simple. I have a device which can get me a random number, hopefully uniformly distributed. Now using this device, I’m to generate other distributions.</p>
<p>I’ve most likely come across fragments of code which does this, without being able to generalize. I can recollect sometime back, when <a href="https://researchweb.iiit.ac.in/~harishkrishna.v/">Harish</a> told me something about taking a cumulative distribution function (CDF) and projecting on the y-axis, but <a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling#Examples">it</a> never really sunk in.</p>
<p>When I took Topics in Machine Learning in my undergrad final year, <a href="https://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/">EXP3</a> algorithm in a Multi-Arm Bandit setting required me to draw an arm with the probabilities of the arms being updated at every step. I figured Jeremy Kun’s code segment was enough under the time constraints - and forgot about it in a while.</p>
<p>Around the start of this month, when I got to using temperature for adjusting a distribution to generate more likely but still diverse samples for a version of <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy</a>’s <a href="https://github.com/karpathy/char-rnn">char-rnn</a>, I stumbled across sampling again, and that was it.</p>
<p>At this point, I strongly suspect <a href="https://www.math.uci.edu/icamp/courses/math77c/demos/hist_eq.pdf">Histogram Equalization</a> which was taught in Digital Image Processing, which I implemented once and left to rust since then is also somehow connected to this.</p>
<p>This post is hence a visit down memory lane, linking and strengthening concepts I most likely missed in the past.</p>
<h1 id="lets-sample">Let’s sample</h1>
<p>I’ll be using pytorch for operating on tensors.</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F
<span class="im">import</span> torch
<span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt
<span class="op">%</span>matplotlib inline</code></pre></div>
<p>For our purposes, we use <code>randn</code> to generate a 1-D tensor of random numbers, on which we apply softmax to convert them to probabilities. These are the probabilites which we’ll use to sample indices.</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">n <span class="op">=</span> <span class="dv">10</span>
acts <span class="op">=</span> torch.randn(n)
<span class="bu">print</span>(acts)
probs <span class="op">=</span> F.softmax(acts, dim<span class="op">=</span><span class="dv">0</span>)</code></pre></div>
<p>The following auxilliary function helps us visualize the distribution.</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python"><span class="kw">def</span> plot_probs_bar(vals):
    H, <span class="op">=</span> vals.size()
    xs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(H))
    vals <span class="op">=</span> vals<span class="op">/</span>vals.<span class="bu">sum</span>()
    ys <span class="op">=</span> vals.tolist()
    plt.ylim(<span class="dv">0</span>, <span class="dv">1</span>)
    plt.bar(xs, ys)

plot_probs_bar(probs)</code></pre></div>
<h2 id="inverse-transform-sampling">Inverse Transform Sampling</h2>
<p>From wikipedia:</p>
<blockquote>
<p>Let <span class="math inline">\(X\)</span> be a random variable whose distribution can be described by the cumulative distribution <span class="math inline">\(F_{X}\)</span>. We want to generate values of which <span class="math inline">\(X\)</span> are distributed according to this distribution.</p>
<p>The inverse transform sampling method works as follows:</p>
<ol style="list-style-type: decimal">
<li>Generate a random number <span class="math inline">\(u\)</span> from the standard uniform distribution in the interval <span class="math inline">\([0,1]\)</span>.<br />
</li>
<li>Compute the value <span class="math inline">\(x\)</span> such that <span class="math inline">\(F_X(x)=u\)</span>.<br />
</li>
<li>Take <span class="math inline">\(x\)</span> to be the random number drawn from the distribution described by <span class="math inline">\(F_x\)</span></li>
</ol>
</blockquote>
<p>Step 2 is a whole lot easier if one can find <span class="math inline">\(F^{-1}\)</span>.</p>
<p>Python’s <code>random.random()</code> samples from a uniform distribution between <span class="math inline">\([0, 1)\)</span>. I guess that’ll suffice as a starting point. Let’s say we have to randomly choose one from <code>n</code> samples, each with probability given by <code>probs[i]</code>, the following function can do the job.</p>
<p><a href="https://github.com/j2kun/exp3/blob/master/probability.py#L7">Jeremy Kun’s draw</a> is very similar.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python"><span class="im">import</span> random

<span class="kw">def</span> draw(probs):
    val <span class="op">=</span> random.random()
    csum <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(probs):
        csum  <span class="op">+=</span> p
        <span class="cf">if</span> csum <span class="op">&gt;</span> val:
            <span class="cf">return</span> i</code></pre></div>
<h2 id="verification">Verification</h2>
<p>How do we verify, in the limit of a lot of samples - if the distribution of selections look like what we intended? We simulate the situation, obviously. I make 10,000 draws to check if the counts when normalized looks like the probability distribution.</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python"><span class="im">from</span> collections <span class="im">import</span> defaultdict

<span class="kw">def</span> check_sampling(probs):
    max_samples <span class="op">=</span> <span class="dv">10000</span>
    counter <span class="op">=</span> defaultdict(<span class="bu">int</span>)
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_samples):
        choice <span class="op">=</span> draw(probs)
        counter[choice] <span class="op">+=</span> <span class="dv">1</span>

    vals <span class="op">=</span> torch.Tensor(<span class="bu">list</span>(counter.values()))
    <span class="cf">return</span> plot_probs_bar(vals)

check_sampling(probs)</code></pre></div>
<p>If the above plot ends up being similar to the one given by just probabilities, it works. Turns out, it works.</p>
<div class="row">
<div class="col">
<div class="figure">
<img src="../static/images/sampling-from-distributions/probs.png" alt="used for sampling" />
<p class="caption">used for sampling</p>
</div>
</div>
<div class="col">
<div class="figure">
<img src="../static/images/sampling-from-distributions/dist.png" alt="obtained after sampling" />
<p class="caption">obtained after sampling</p>
</div>
</div>
</div>
<h1 id="karpathys-char-rnn">Karpathy’s Char RNN</h1>
<p>The output layer of Karpathy’s char-rnn predicts probability of certain given classes. For a task like translation or nearly anything involving classification, <code>argmax</code> of the probabilities is the obvious choice. But since we’re hallucinating valid sequences - we have to randomly sample from the most probable choices.</p>
<p>For this, we skew the distribution to increase the probabilities of likely samples, and decreasing the probabilties of unlikely samples. Temperature acts as the control knob here, and the following illustration (hopefully) convinces why.</p>
<h2 id="temperature">Temperature</h2>
<p><span class="math display">\[ \mathrm{softmax}(x, T)  = \frac{e^{x_i/T}}{\sum{e^{x_i/T}}}\]</span></p>
<p>I’ll model my <code>TSoftmax</code> as a functor, which uses pytorch’s nn.Module.</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python"><span class="kw">class</span> TSoftmax(nn.Module):
    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, temperature):
        <span class="bu">super</span>().<span class="fu">__init__</span>()
        <span class="va">self</span>.T <span class="op">=</span> temperature

    <span class="kw">def</span> forward(<span class="va">self</span>, tensor):
        <span class="co"># Take all samples, divide them by T, pass through exp(x)</span>
        entries <span class="op">=</span> tensor.data.view(<span class="op">-</span><span class="dv">1</span>).div(<span class="va">self</span>.T).exp()
        <span class="cf">return</span> entries<span class="op">/</span>entries.<span class="bu">sum</span>()</code></pre></div>
<p>If <span class="math inline">\(T = 1\)</span>, the above reduces to simple softmax. So, let’s try a few samples <span class="math inline">\(T \lt 1\)</span>, and <span class="math inline">\(T \gt 1\)</span>, see how the probability distribution we use to sample from changes.</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">m <span class="op">=</span> <span class="dv">4</span>
less_than_one <span class="op">=</span> [<span class="dv">2</span><span class="op">**</span>(<span class="op">-</span>b) <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, m)]
greater_than_one <span class="op">=</span> [<span class="dv">2</span><span class="op">**</span>(b) <span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, m)]

tvals <span class="op">=</span> <span class="bu">list</span>(<span class="bu">reversed</span>(less_than_one)) <span class="op">+</span> [<span class="dv">1</span>]  <span class="op">+</span> greater_than_one
n_tvals <span class="op">=</span> <span class="bu">len</span>(tvals)

plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">30</span>))

<span class="cf">for</span> i, T <span class="kw">in</span> <span class="bu">enumerate</span>(tvals):
    v <span class="op">=</span> i<span class="op">+</span><span class="dv">1</span>
    ax1 <span class="op">=</span> plt.subplot(n_tvals,<span class="dv">1</span>,v)
    ax1.set_title(<span class="st">&quot;t = </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(T))
    transform <span class="op">=</span> TSoftmax(temperature <span class="op">=</span> T)
    tprobs <span class="op">=</span> transform(acts)
    plot_probs_bar(tprobs)</code></pre></div>
<div class="figure">
<img src="../static/images/sampling-from-distributions/temperature_variance.png" alt="temperature variation" />
<p class="caption">temperature variation</p>
</div>
<p>Increase in <span class="math inline">\(T\rightarrow \infty\)</span> leads to uniform distribution, decrease in <span class="math inline">\(T \rightarrow 0\)</span> leads to argmax situation. Thus, our good values for hallucinating could be somewhere in <span class="math inline">\([0.5, 0.7]\)</span>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>except he uses <code>weights</code> which aren’t normalized and uses a choice from a uniform distribution of <code>[0, sum(weights)]</code> instead. Most likely an overkill, imo.<a href="#fnref1">↩</a></p></li>
</ol>
</div></div>
</div>
<div class="row">
    <div class="post-info col">
        2018-07-18
        
        by Jerin Philip
        
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
