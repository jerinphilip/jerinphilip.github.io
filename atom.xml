<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Jerin Philip's blog</title>
    <link href="http://jerinphilip.github.io/atom.xml" rel="self" />
    <link href="http://jerinphilip.github.io" />
    <id>http://jerinphilip.github.io/atom.xml</id>
    <author>
        <name>Jerin Philip</name>
        <email>jerinphilip@live.in</email>
    </author>
    <updated>2023-08-12T00:00:00Z</updated>
    <entry>
    <title>slimt: Making of</title>
    <link href="http://jerinphilip.github.io/posts/making-of-slimt.html" />
    <id>http://jerinphilip.github.io/posts/making-of-slimt.html</id>
    <published>2023-08-12T00:00:00Z</published>
    <updated>2023-08-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="slimt: Making of" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/making-of-slimt.html" />
        <title>slimt: Making of</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>slimt: Making of</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Aug 12, 2023</div>

         

         
            <div><a href="../tags/posts/mt.html">mt</a>, <a href="../tags/posts/ml.html">ml</a>, <a href="../tags/posts/bergamot.html">bergamot</a></div>
         
    </div>
</div>

<div class="row">
    <div class="toc col-md-4 col-sm-12 order-md-second"><h6>Outline</h6><ul>
<li><a href="#the-task">The task</a></li>
<li><a href="#approach">Approach</a><ul>
<li><a href="#python">Python</a></li>
<li><a href="#debugger">Debugger</a></li>
<li><a href="#breakthrough-hack">Breakthrough hack</a></li>
<li><a href="#tracing-execution">Tracing execution</a></li>
</ul></li>
<li><a href="#finishing-up">Finishing up</a></li>
<li><a href="#references">References</a></li>
</ul></div><div class="col-md-8 col-sm-12 post-content order-md-first"><p>This post is about <a href="https://github.com/jerinphilip/slimt">slimt</a>, software I have been working on for the past 10-days or so. It’s slim machine-translation (MT) inference code. It was fun doing it, and some bits along the journey documentable.</p>
<p><strong>Flashback</strong> One of the first things I was tasked with starting undergraduate research at university was to move an Optical Character Recognition (OCR) library from C++ to Python. This was circa 2015 - PyTorch was new, TensorFlow was dark magic research seniors rejected in favour of PyTorch. But surprise, What the lab had powering it’s document research efforts was <a href="https://sourceforge.net/p/rnnl/wiki/Home/">rnnlib</a>. Story goes that some researcher managed to get it to work, and it’s been powering text-recognition ever since. Training was on CPUs, the library predated <a href="https://www.image-net.org/challenges/LSVRC/2012/">ImageNet</a>. But Bidirectional LSTMs were the best the lab had back then.</p>
<p>I went ahead, checked the source-code and reported back we should just do it in PyTorch. Back then I’d blame lack of documentation and a missing map to the source-code. My advisor, unsurprisingly held on to the requirement - <em>it’s just a bunch of matrix multiplies, why is it so hard to move from C++ to Python?</em> Basically I had to find the parts where the said matrices showed up in code and use it - except it was taking long. I tried a lot of stuff - used Doxygen to generate diagrams, opened up source files based on names. Tried to step through the GDB debugger to find which lines of source where getting executed. I’d be naive and go back and report these things to my advisor who mostly works in computer vision to see blank faces all the time. The efforts didn’t succeed (or we did not wait for it rather.</p>
<p>On the bright side, I did learn quite a bit of C++ tooling and developed some interest in the area. Fast forward 5 years, I have moved from Python machine learning training to AI inference and learning to write machine learning frameworks and had come full circle to a similar task.</p>
<h2 id="the-task">The task</h2>
<p>A task pending long in my to-do list is polish <a href="https://github.com/jerinphilip/lemonade">lemonade</a>, a translation input method engine for easier use. I was procrastinating, until contemporary efforts such as <a href="https://github.com/ggerganov/ggml">ggerganov/ggml</a> and <a href="https://github.com/karpathy/llama2.c">karpathy/llama2.c</a> got me thinking - wouldn’t my translation inference only need specialized code for just one class of models?</p>
<p>The models are transformers with only minor modifications. The <a href="https://github.com/browsermt/students/tree/master/deen">tiny11</a> class of models are actually quite small after 8-bit quantization - <code>ende.student.tiny11</code> is 17MB on my system. marian-dev is source-code I’d been lurking around for about 2 years.</p>
<p>bergamot-translator builds on top of marian-dev and uses the inference code-path from marian-dev. While marian is a a capable neural network library with focus on machine translation, all the bells and whistles that come with it are not necessary to run inference on client-machines (e.g: autograd, multiple sequence-to-sequence architecture support, beam-search). When I started marian used to be a monstrous 30 minute compile on my laptop, which I brought down using ccache and <a href="../posts/ccache.html">wrote about</a> - stripping the code to only inference would make that look lame and useless. I would not even need ccache anymore.</p>
<p>So - write an own transformer forward pass, bring compile-times down like crazy, reduce source-complexity*, minimize dependencies, get something of possible value out of it - lot of boxes were checking themselves. Be a shame if I decided not to give it a go, especially given all the free-time I have.</p>
<p>So, there’s a clear destination - the approach and path still had elements of uncertainties. This is essentially the same task as 7 years ago - produce new inference code for a model trained from the library outside.</p>
<h2 id="approach">Approach</h2>
<p>I had a plan in mind - (1) inspect the model binary, (2) step through a debugger checking activated paths, understand the logic behind and then implement. Copying code was okay (and a requirement, since the code computing float operations had to match), so we’d have it slightly easier.</p>
<h3 id="python">Python</h3>
<p>I thought of projecting 8-bit <code>int</code> to 32-bit <code>float</code> and doing the operations in PyTorch for a faster first iteration. Once there was clarity in the network architecture and I managed to realize it fast and verify with Python, implementing C++ should be easier - was the thinking.</p>
<p>I <a href="https://gist.github.com/jerinphilip/670495fca010b2cde4f34091fcb1f5d3">wrote a quick script</a> to load the model in Python.</p>
<p><details><summary> <code>model.bin</code> (truncated) </summary></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">Item(Wemb, intgemm8, Shape([<span class="dv">32000</span>, <span class="dv">256</span>]), <span class="dv">8192256</span>)
Item(Wemb_QuantMultA, intgemm8, Shape([<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dv">256</span>)
Item(decoder_ff_logit_out_b, float32, Shape([<span class="dv">1</span>, <span class="dv">32000</span>]), <span class="dv">128000</span>)
Item(decoder_l1_context_Wk, intgemm8, Shape([<span class="dv">256</span>, <span class="dv">256</span>]), <span class="dv">65792</span>)
...
Item(decoder_l1_ffn_b2, float32, Shape([<span class="dv">1</span>, <span class="dv">256</span>]), <span class="dv">1024</span>)
Item(decoder_l1_ffn_ffn_ln_bias, float32, Shape([<span class="dv">1</span>, <span class="dv">256</span>]), <span class="dv">1024</span>)
Item(decoder_l1_ffn_ffn_ln_scale, float32, Shape([<span class="dv">1</span>, <span class="dv">256</span>]), <span class="dv">1024</span>)
Item(decoder_l1_rnn_W, intgemm8, Shape([<span class="dv">256</span>, <span class="dv">256</span>]), <span class="dv">65792</span>)
Item(decoder_l1_rnn_W_QuantMultA, float32, Shape([<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dv">256</span>)
...
Item(decoder_l2_context_Wq, intgemm8, Shape([<span class="dv">256</span>, <span class="dv">256</span>]), <span class="dv">65792</span>)
Item(decoder_l2_context_Wq_QuantMultA, float32, Shape([<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dv">256</span>)
Item(decoder_l2_context_Wv, intgemm8, Shape([<span class="dv">256</span>, <span class="dv">256</span>]), <span class="dv">65792</span>)
Item(decoder_l2_context_Wv_QuantMultA, float32, Shape([<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dv">256</span>)
Item(decoder_l2_context_bk, float32, Shape([<span class="dv">1</span>, <span class="dv">256</span>]), <span class="dv">1024</span>)
Item(decoder_l2_context_bo, float32, Shape([<span class="dv">1</span>, <span class="dv">256</span>]), <span class="dv">1024</span>)
...
Item(encoder_l1_ffn_W1, intgemm8, Shape([<span class="dv">256</span>, <span class="dv">1536</span>]), <span class="dv">393472</span>)
Item(encoder_l1_ffn_W1_QuantMultA, float32, Shape([<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dv">256</span>)
Item(encoder_l1_ffn_b1, float32, Shape([<span class="dv">1</span>, <span class="dv">1536</span>]), <span class="dv">6144</span>)
...
Item(encoder_l1_ffn_ffn_ln_bias, float32, Shape([<span class="dv">1</span>, <span class="dv">256</span>]), <span class="dv">1024</span>)
Item(encoder_l1_ffn_ffn_ln_scale, float32, Shape([<span class="dv">1</span>, <span class="dv">256</span>]), <span class="dv">1024</span>)
Item(encoder_l1_self_Wk, intgemm8, Shape([<span class="dv">256</span>, <span class="dv">256</span>]), <span class="dv">65792</span>)
Item(encoder_l1_self_Wk_QuantMultA, float32, Shape([<span class="dv">1</span>, <span class="dv">1</span>]), <span class="dv">256</span>)</code></pre></div>
<p></details></p>
<p>Some things make sense. There are attention layers, feed-forward networks, RNNs, and output layer, some layernorm. I have the flat data, not the structure. I was hoping to recover neural network structure from a research or system-description paper of some sort. I checked <span class="citation">Kim et al. (<a href="#ref-kim2019research">2019</a>)</span>, there’s some mention of the RNN (SSRU), but a network diagram is missing. I could do further level searches on papers this paper was referring to. Without the structure, it would be difficult to reproduce in Python. I’ll also need some tooling to verify I’m on the right path. So I decided to see if throwing the debugger and stepping would be of any help. After-all, code was <a href="https://www.commitstrip.com/wp-content/uploads/2016/08/Strip-Les-specs-cest-du-code-650-finalenglish.jpg">absolute truth</a>.</p>
<h3 id="debugger">Debugger</h3>
<p>IDEs back in 2015 when I took on the previous porting had as well I’d guess. But back then I knew only <code>gdb</code> and mostly used vim. I still do use vim most of the time, and VSCode with an vim emulation layer. VSCode has a nice debugger. I currently use <a href="https://vscodium.com/">VSCodium</a>, which is a fork. Internally, VSCode uses <code>gdb</code>, but it’s easier to move around and inspect.</p>
<p>Starting with bergamot’s <code>translateBatch</code> entrypoint, I jumped to function definitions and checked a few call-stacks. Created a script to link and save, as I unrolled the code while porting. See a few examples below.</p>
<details><summary> Embedding (index_select) </summary>
<pre>
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/tensors/cpu/tensor_operators.cpp#L565">marian::cpu::CopyRows</a>(marian::Tensor out_, const marian::Tensor in_, const marian::Tensor indices) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/tensors/tensor_operators.h#L217">marian::CopyRows</a>(marian::Tensor arg1, const marian::Tensor arg2, const marian::Tensor arg3) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_binary.h#L672">marian::RowsNodeOp::forwardOps()::{lambda()#1}::operator()() const</a>(const struct {...} * const __closure) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node.h#L64">marian::Node::runForward(std::vector<std::function<void ()>, std::allocator<std::function<void ()> > > const&)</a>(marian::Node * const this, const marian::NodeOps & ops) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node.cpp#L67">marian::Node::forward</a>(marian::Node * const this) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/expression_graph.cpp#L114">marian::ExpressionGraph::forward</a>(marian::ExpressionGraph * const this, std::__cxx11::list<IntrusivePtr<marian::Chainable<IntrusivePtr<marian::TensorBase> > >, std::allocator<IntrusivePtr<marian::Chainable<IntrusivePtr<marian::TensorBase> > > > > & forwardTape, bool finalPass) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/expression_graph.cpp#L101">marian::ExpressionGraph::forwardNext</a>(marian::ExpressionGraph * const this) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/expression_graph.h#L246">marian::ExpressionGraph::forward</a>(marian::ExpressionGraph * const this) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/beam_search.cpp#L451">marian::BeamSearch::search</a>(marian::BeamSearch * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/translation_model.cpp#L186">marian::bergamot::TranslationModel::translateBatch</a>(marian::bergamot::TranslationModel * const this, marian::bergamot::Workspace & workspace, marian::bergamot::Batch & batch)</pre>
<p></details></p>
<details><summary> Positional Embeddings </summary>
<pre>
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_initializers.cpp#L216">marian::inits::sinusoidalPositionEmbeddings</a>(int start) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L105">marian::Transformer<marian::EncoderBase>::addPositionalEmbeddings</a>(const marian::Transformer<marian::EncoderBase> * const this, marian::Expr input, int start, bool trainPosEmbeddings) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L117">marian::Transformer<marian::EncoderBase>::addSpecialEmbeddings</a>(const marian::Transformer<marian::EncoderBase> * const this, marian::Expr input, int start) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L555">marian::EncoderTransformer::apply</a>(marian::EncoderTransformer * const this, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L543">marian::EncoderTransformer::build</a>(marian::EncoderTransformer * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/encoder_decoder.cpp#L187">marian::EncoderDecoder::startState</a>(marian::EncoderDecoder * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/costs.h#L357">marian::models::Stepwise::startState</a>(marian::models::Stepwise * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/scorers.h#L126">marian::ScorerWrapper::startState</a>(marian::ScorerWrapper * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/beam_search.cpp#L280">marian::BeamSearch::search</a>(marian::BeamSearch * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/translation_model.cpp#L186">marian::bergamot::TranslationModel::translateBatch</a>(marian::bergamot::TranslationModel * const this, marian::bergamot::Workspace & workspace, marian::bergamot::Batch & batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/service.cpp#L149">operator()</a>(const struct {...} * const __closure)
</pre>
<p></details></p>
<details><summary> Encoder forward </summary>
<pre>
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L548">marian::EncoderTransformer::apply</a>(marian::EncoderTransformer * const this, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L540">marian::EncoderTransformer::build</a>(marian::EncoderTransformer * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/encoder_decoder.cpp#L187">marian::EncoderDecoder::startState</a>(marian::EncoderDecoder * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/costs.h#L357">marian::models::Stepwise::startState</a>(marian::models::Stepwise * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/scorers.h#L126">marian::ScorerWrapper::startState</a>(marian::ScorerWrapper * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/beam_search.cpp#L280">marian::BeamSearch::search</a>(marian::BeamSearch * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/translation_model.cpp#L186">marian::bergamot::TranslationModel::translateBatch</a>(marian::bergamot::TranslationModel * const this, marian::bergamot::Workspace & workspace, marian::bergamot::Batch & batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/service.cpp#L149">operator()</a>(const struct {...} * const __closure)
</pre>
<p></details></p>
<details><summary> transposed log mask </summary>
<pre>
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L132">marian::Transformer<marian::EncoderBase>::transposedLogMask</a>(marian::Expr mask) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L569">marian::EncoderTransformer::apply</a>(marian::EncoderTransformer * const this, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L540">marian::EncoderTransformer::build</a>(marian::EncoderTransformer * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/encoder_decoder.cpp#L187">marian::EncoderDecoder::startState</a>(marian::EncoderDecoder * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/costs.h#L357">marian::models::Stepwise::startState</a>(marian::models::Stepwise * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/scorers.h#L126">marian::ScorerWrapper::startState</a>(marian::ScorerWrapper * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/beam_search.cpp#L280">marian::BeamSearch::search</a>(marian::BeamSearch * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/translation_model.cpp#L186">marian::bergamot::TranslationModel::translateBatch</a>(marian::bergamot::TranslationModel * const this, marian::bergamot::Workspace & workspace, marian::bergamot::Batch & batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/service.cpp#L149">operator()</a>(const struct {...} * const __closure)
</pre>
<p></details></p>
<details> <summary> postprocess: Applied after each FFN (Takes care of skip, off dropout and LayerNorm) </summary>
<pre>
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L181">marian::Transformer<marian::EncoderBase>::postProcess</a>(const marian::Transformer<marian::EncoderBase> * const this, std::string prefix, std::string ops, marian::Expr input, marian::Expr prevInput, float dropProb) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L372">marian::Transformer<marian::EncoderBase>::LayerAttention</a>(marian::Transformer<marian::EncoderBase> * const this, std::string prefix, marian::Expr input, const marian::Expr & keys, const marian::Expr & values, const marian::Expr & mask, int dimHeads, bool cache, bool saveAttentionWeights) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L575">marian::EncoderTransformer::apply</a>(marian::EncoderTransformer * const this, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/transformer.h#L540">marian::EncoderTransformer::build</a>(marian::EncoderTransformer * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/encoder_decoder.cpp#L187">marian::EncoderDecoder::startState</a>(marian::EncoderDecoder * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/models/costs.h#L357">marian::models::Stepwise::startState</a>(marian::models::Stepwise * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/scorers.h#L126">marian::ScorerWrapper::startState</a>(marian::ScorerWrapper * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/translator/beam_search.cpp#L280">marian::BeamSearch::search</a>(marian::BeamSearch * const this, marian::Ptr graph, marian::Ptr batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/translation_model.cpp#L186">marian::bergamot::TranslationModel::translateBatch</a>(marian::bergamot::TranslationModel * const this, marian::bergamot::Workspace & workspace, marian::bergamot::Batch & batch) 
<a href="../home/jerin/code/bergamot-translator/src/translator/service.cpp#L149">operator()</a>(const struct {...} * const __closure)
</pre>
<p></details></p>
<p>I quickly discovered the code-paths that were consistently getting activated when I ran input through, and discovered some of the realizations of the transformer code. but this process was slower than what I’d wished for. My plan was viable, just not fast enough. I needed something faster.</p>
<h3 id="breakthrough-hack">Breakthrough hack</h3>
<p>During this process, I discovered the <a href="https://github.com/browsermt/marian-dev/blob/aa0221e687fe8b3b69b5bb64279d4349663ad410/src/common/definitions.h#L14"><code>NodeOp</code></a> macro, which looked as follows.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="pp">#define NodeOp(op) [=]() { op; }</span></code></pre></div>
<p>For some reason, marian was using this to describe forward and backward in the computational graph, mostly consistently. Since searching for, finding the macro, manually putting the breakpoint wasn’t cutting it - I quickly googled if I could programmatically stop for debugger. Turns out, I can - <code>std::raise(SIGTRAP)</code>.</p>
<p>The operating system notifies the debugger on SIGTRAP signal (if no debugger, the program simply exits). The debugger can map the instruction pointer to the line in source both ways (provided compiled with <code>-g</code> and ideally, <code>-O0</code>, which is <code>-DCMAKE_BUILD_TYPE=Debug</code>).</p>
<p>I set a programmatic breakpoint and tried to extract call-stack programmatically as well:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="pp">#define NodeOp(op)                                                  </span>\
<span class="pp">  [=]() {                                                           </span>\
<span class="pp">    std::raise(SIGTRAP);                                            </span>\
<span class="pp">    std::string callstack = marian::getCallStack(/*skipLevels=*/3); </span>\
<span class="pp">    std::cerr &lt;&lt; callstack &lt;&lt; &quot;</span><span class="er">\</span><span class="pp">n&quot;;                                 </span>\
<span class="pp">    std::cerr &lt;&lt; </span><span class="ot">__PRETTY_FUNCTION__</span><span class="pp"> ;                              </span>\
<span class="pp">    std::cerr &lt;&lt; &quot; &quot; &lt;&lt; </span><span class="ot">__FILE__</span><span class="pp"> &lt;&lt; &quot;:&quot;;                            </span>\
<span class="pp">    std::cerr &lt;&lt; </span><span class="ot">__LINE__</span><span class="pp"> &lt;&lt; &quot;</span><span class="er">\</span><span class="pp">n&quot;;                                  </span>\
<span class="pp">    op;                                                             </span>\
<span class="pp">  }</span></code></pre></div>
<p>I quickly realized I no longer needed the <code>SIGTRAP</code> to know which functions where getting called. Dropping it and just using <code>__PRETTY_FUNCTION__</code> information identified ops using <code>NodeOp(...)</code>.</p>
<details> <summary> Ops (click to expand) </summary>
<pre>
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/tensors/cpu/integer_common.h#L46">marian::cpu::integer::fetchAlphaFromModelNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_binary.h#L419">marian::DotBatchedNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_binary.h#L735">marian::GatherNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_binary.h#L1252">marian::HighwayNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_binary.h#L1200">marian::LayerNormalizationOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_unary.h#L450">marian::LogSoftmaxNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_unary.h#L728">marian::NegNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_binary.h#L831">marian::PlusNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_unary.h#L284">marian::ReLUNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_binary.h#L672">marian::RowsNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_unary.h#L42">marian::ScalarAddNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_unary.h#L100">marian::ScalarMultNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_unary.h#L425">marian::SoftmaxNodeOp::forwardOps()::&lt;lambda&gt;</a>() 
<a href="https://github.com/jerinphilip/marian/blob/8c4170fa08c46df1cf4c987e493b7a3772c380b3/src/graph/node_operators_unary.h#L747">marian::TransposeNodeOp::forwardOps()::&lt;lambda&gt;</a>()
</pre>
<p></details></p>
<p>The first discovered Ops were not exhaustive. There are some functions that simply use a capturing lambda and not the macro. I don’t think the authors of the macro ever intended it to be used this way. To my surprise, I discovered <code>NodeOp</code> was even <a href="https://marian-nmt.github.io/docs/api">mentioned by a documentation effort</a>. Macros are supposed to be <a href="https://stackoverflow.com/questions/14041453/why-are-preprocessor-macros-evil-and-what-are-the-alternatives">bad and evil</a> per established wisdom.</p>
<p>If as reader, you feel that this article is jumping all around the place - know that it is an accurate reflection my mental state and uncertainty regarding the target at this point. But from here-on, I had clarity.</p>
<h3 id="tracing-execution">Tracing execution</h3>
<p>I’m mostly lurking and operating in the compilers intersection machine-learning space now. So far I’ve also followed the <a href="https://minitorch.github.io/">minitorch</a> tutorial twice - once in Python and once in C++ to know what a computation graph is and how to build autograd. This puts me in place with certain theory and understanding that could make life further simpler for me.</p>
<p>All that aside, we will consider a toy language of data-types being <code>Expr</code>, indicating an expression (more specifically, the result of an expression). We can do the following with <code>Expr</code>s for an example.</p>
<p>We’ll make a simplified definition of <code>Expr</code> as follows:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">// Expr lhs = Op(rhs[0], rhs[1], ...)</span>
<span class="kw">struct</span> Expr { 
   <span class="dt">float</span>* value; <span class="co">// Holds underlying </span>
   storage <span class="dt">size_t</span> size;  <span class="co">// Size of the storage, in bytes.</span>

   <span class="kw">using</span> Operands = <span class="bu">std::</span>vector&lt;Expr&gt;; 
   Operands rhs; <span class="co">// operands that populate the result value.</span>

   <span class="kw">using</span> Op  = <span class="bu">std::</span>function&lt;<span class="dt">void</span>(<span class="dt">void</span>)&gt;; 
   <span class="kw">using</span> Ops = <span class="bu">std::</span>vector&lt;Op&gt;;

   Ops forward() { 
     <span class="kw">auto</span> op = [=](){ 
       <span class="co">// Open up rhs, apply intended function.</span>
       <span class="co">// write to value.  </span>
     }; 
     <span class="cf">return</span> { op }; 
   }

   <span class="dt">float</span> *grad; <span class="co">// Same size as value, holds gradient</span>

   <span class="co">// Operates on grad, after receiving gradients from Expr(s) ahead.  </span>
   Ops backward(<span class="dt">float</span> *grad_from_successor_node);  };</code></pre></div>
<p>Since we’re doing inference, we’re not interested in <code>backward</code> and <code>grad</code>.</p>
<p>Consider the expression as an LHS which is obtained by some operation on the rhs operands.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">Expr x = ones(<span class="dv">2</span>, <span class="dv">2</span>);
Expr y = zero(<span class="dv">2</span>, <span class="dv">2</span>);
Expr z = x + y;</code></pre></div>
<p>Consider <code>z</code>, which is a result of <code>+</code> on <code>[x, y]</code>, <code>Expr</code> would be as follows:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="kw">struct</span> Add: <span class="kw">public</span> Expr {
<span class="kw">public</span>:
  <span class="co">// ...</span>
  Ops forward() {
    <span class="co">// Open up rhs, apply intended function.</span>
    <span class="co">// write to value.</span>
    <span class="co">// x, y are available in rhs.</span>
    Op add = [=](){
      <span class="cf">for</span>(<span class="dt">size_t</span> i = <span class="dv">0</span>; i &lt; size; i++){
          value[i] = rhs[<span class="dv">0</span>].value[i] + rhs[<span class="dv">1</span>].value[i];
      }
    };
    <span class="cf">return</span> { add };
  }
};</code></pre></div>
<p>Note that we’re only recording that so and so operations must be done using so and so storage locations - <a href="https://en.wikipedia.org/wiki/Thunk">thunks</a>. We’ve not actually executed them yet. Execution would look as follows:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="co">// Compute loss</span>
loss = f(rhs1, rhs2, ...);

<span class="co">// Note: Topological order begins from first expr, it is the</span>
<span class="co">// reverse-topological-order that starts with loss. </span>
<span class="bu">std::</span>vector&lt;Expr&gt; order = topological_order(loss); 

<span class="co">// Run forward ops</span>
<span class="cf">for</span>(<span class="kw">auto</span> expr: order){
  forward_ops = expr-&gt;forward();
  <span class="co">// Execute functions, ends up in order of construction.</span>
  forward_ops(); 
}</code></pre></div>
<p>Okay.. what’s the point of all this? Turns out <code>NodeOp</code> being used to package the thunk means I can use <code>NodeOp</code> macro to add a pre and post hook to the statements. This means by the following modification, I can inspect the values of <code>lhs</code> (value) before and after, and also inspect the state of <code>rhs</code> (operands) during the op.</p>
<p>The modification I’m looking for looks like:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="pp">#define WrapStatementInLambda(statement)                            </span>\
<span class="pp">  [=]() {                                                           </span>\
<span class="pp">    // Open up and inspect value (Before op)                        </span>\
<span class="pp">    // (Not usually required.)                                      </span>\
<span class="pp">                                                                    </span>\
<span class="pp">    // Extra local information                                      </span>\
<span class="pp">    std::cerr &lt;&lt; </span><span class="ot">__PRETTY_FUNCTION__</span><span class="pp"> ;                              </span>\
<span class="pp">    std::cerr &lt;&lt; &quot; &quot; &lt;&lt; </span><span class="ot">__FILE__</span><span class="pp"> &lt;&lt; &quot;:&quot;;                            </span>\
<span class="pp">    std::cerr &lt;&lt; </span><span class="ot">__LINE__</span><span class="pp"> &lt;&lt; &quot;</span><span class="er">\</span><span class="pp">n&quot;;                                  </span>\
<span class="pp">                                                                    </span>\
<span class="pp">    // Execute the operation                                        </span>\
<span class="pp">    // Just leave the arg to unroll statements wrapped by macro.    </span>\
<span class="pp">                                                                    </span>\
<span class="pp">    statement;                                                      </span>\
<span class="pp">                                                                    </span>\
<span class="pp">    // save value (lhs), rhs[0], rhs[1] ... to disk(?)              </span>\
<span class="pp">  }</span>


<span class="pp">#define NodeOp(op)  WrapStatementInLambda(op)</span></code></pre></div>
<p>The rich-version I eventually ended up using is available in <a href="https://github.com/jerinphilip/slimt/blob/main/scripts/marian-trace-gen.h">slimt/marian-trace-gen.h</a>. I was also able to extract shape metadata at runtime, some name and unique-identifier information that was stored in the values. The unique-id meant I could conditionally stop during execution based on the identifier value. The macro-modification is a one-off throwaway item, but can be refined to trace the exact final operations executed by a marian forward pass and backward-pass if need be. There were a few <code>Expr</code>s not using <code>NodeOp</code>, but were <a href="https://github.com/jerinphilip/marian/compare/dev...jerinphilip:marian:tracing">easy to tame</a>.</p>
<p>My traces looked like below:</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="fu">file:</span><span class="at"> </span><span class="st">&quot;/home/jerin/code/bergamot-translator/3rd_party/marian-dev/src/graph/node_operators_binary.h&quot;</span>
<span class="fu">line:</span><span class="at"> 836</span>
<span class="fu">fn:</span><span class="at"> </span><span class="st">&quot;marian::PlusNodeOp::forwardOps()::&lt;lambda()&gt;&quot;</span>
<span class="fu">op:</span><span class="at"> </span><span class="kw">{</span> Element(_1 = _2 + _3, val_, child(0)-&gt;val(), child(1)-&gt;val()) <span class="kw">}</span>
<span class="fu">before:</span><span class="at"> var_45 float32 [2x8x4x4]</span>
<span class="fu">after:</span><span class="at"> var_45 float32 [2x8x4x4] var_45-PlusNodeOp-float32_2x8x4x4-lhs.bin</span>
<span class="fu">operands:</span><span class="at"> </span>
  <span class="kw">-</span> var_44 float32 <span class="kw">[</span>2x8x4x4<span class="kw">]</span> var_45-PlusNodeOp-float32_2x8x4x4-rhs0-float32_2x8x4x4.bin
  <span class="kw">-</span> var_16 float32 <span class="kw">[</span>2x1x1x4<span class="kw">]</span> var_45-PlusNodeOp-float32_2x8x4x4-rhs1-float32_2x1x1x4.bin


<span class="fu">file:</span><span class="at"> </span><span class="st">&quot;/home/jerin/code/bergamot-translator/3rd_party/marian-dev/src/graph/node_operators_unary.h&quot;</span>
<span class="fu">line:</span><span class="at"> 425</span>
<span class="fu">fn:</span><span class="at"> </span><span class="st">&quot;marian::SoftmaxNodeOp::forwardOps()::&lt;lambda()&gt;&quot;</span>
<span class="fu">op:</span><span class="at"> </span><span class="kw">{</span> Softmax(val_, child(0)-&gt;val()) <span class="kw">}</span>
<span class="fu">before:</span><span class="at"> var_46 float32 [2x8x4x4]</span>
<span class="fu">after:</span><span class="at"> var_46 float32 [2x8x4x4] var_46-SoftmaxNodeOp-float32_2x8x4x4-lhs.bin</span>
<span class="fu">operands:</span><span class="at"> </span>
  <span class="kw">-</span> var_45 float32 <span class="kw">[</span>2x8x4x4<span class="kw">]</span> var_46-SoftmaxNodeOp-float32_2x8x4x4-rhs0-float32_2x8x4x4.bin</code></pre></div>
<p>The full execution trace is available <a href="https://gist.github.com/jerinphilip/e3ff5a29c55a554849c0e5a3ed4ca3fa">here</a>.</p>
<p>Notice, how I had the LHS and RHS for the ops saved onto-disk under unique names. This meant I could even unit-test my ops. The trace was <em>linear</em> unlike the nested functions I’d been hopping through back and forth, context switching. The linear nature made the underlying operations easier to reason with. With some domain knowledge it’s easy to recognize the above code as the softmax in attention after addition of mask for pad-tokens.</p>
<p>At this point, I knew what I wanted was realizable at a pace I was happy with. The problem was more or less solved inside my head. I had all the missing pieces, and a really small chance of failure. Note that I hadn’t completed the solution yet, I’ve just figured out the solution.</p>
<h2 id="finishing-up">Finishing up</h2>
<p>I had made the process mechanical. I traversed the trace porting code step-by-step, checking LHS and RHS tensors matched what I computed using my ported code. I built some verification convenience functions to check as I progressed as well, which looked within source as follows.</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp">Tensor &amp;encoder_out = x;
VERIFY_MATCH(encoder_out,
             <span class="st">&quot;var_394-LayerNormalizationOp-float32_1x2x4x256-lhs.bin&quot;</span>);
<span class="cf">return</span> <span class="va">decoder_</span>.decode(encoder_out, mask, batch.words());</code></pre></div>
<p>I hit some hiccups at 8-bit matrix multiply using intgemm (and ruy later on) in marian. But the saved tensor input/output pairs I left for myself via the tracing helped a lot.</p>
<p>That I was familiar with the components help speed things up a bit. Some corners were cut, but no problem - we can fix it slowly if need be. There is a lot more room or optimizations. I am currently trying my hands at compilers and parallel-programming, and weak-baselines should be opportunity to learn more things on the way.</p>
<p>Changing <code>Node.h</code> and recompiling I’d estimate take 20+ minutes on my laptop, which is enough time to walk away elsewhere while developing tracing scripts - so the new more powerful PC helped a bit.</p>
<p>The source-code is <a href="https://github.com/jerinphilip/slimt">made public</a> on achieving bare-minimum functionality on <code>x86_64</code>, and have <a href="https://github.com/jerinphilip/slimt/pull/2">a PR open</a> to support <code>aarch64</code>. Some code that I wrote for <a href="https://github.com/jerinphilip/MozIntGemm">ARM support for Mozilla</a> back in the day and the experience ended up helping. As of now I am aware of KDE using bergamot’s models in <a href="https://invent.kde.org/libraries/ktextaddons/-/tree/master/texttranslator/translator/plugins/bergamot">KTextAddons</a>. A refined version of this could be useful to Mozilla, who I know to be using only tiny11 models.</p>
<p>This post mostly deals with the development process. I hope to write in the future about the actual technical and math content surrounding these models.</p>
<h2 id="references" class="unnumbered">References</h2>
<div id="refs" class="references">
<div id="ref-kim2019research">
<p>Young Jin Kim, Marcin Junczys-Dowmunt, Hany Hassan Awadalla, Alham Fikri Aji, Kenneth Heafield, Roman Grundkiewicz, and Nikolay Bogoychev. 2019. From research to production and back: Ludicrously fast neural machine translation. In <em>Proceedings of the 3rd workshop on neural generation and translation</em>, pages 280–288.</p>
</div>
</div></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>Alignment manipulations for pivoting in MT</title>
    <link href="http://jerinphilip.github.io/posts/alignment-manipulations-pivoting-mt.html" />
    <id>http://jerinphilip.github.io/posts/alignment-manipulations-pivoting-mt.html</id>
    <published>2023-07-24T00:00:00Z</published>
    <updated>2023-07-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="Alignment manipulations for pivoting in MT" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/alignment-manipulations-pivoting-mt.html" />
        <title>Alignment manipulations for pivoting in MT</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Alignment manipulations for pivoting in MT</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Jul 24, 2023</div>

         

         
            <div><a href="../tags/posts/mt.html">mt</a>, <a href="../tags/posts/ml.html">ml</a>, <a href="../tags/posts/bergamot.html">bergamot</a></div>
         
    </div>
</div>

<div class="row">
    <div class="toc col-md-4 col-sm-12 order-md-second"><h6>Outline</h6><ul>
<li><a href="#attention">Attention</a></li>
<li><a href="#pivoting">Pivoting</a><ul>
<li><a href="#same-vocabulary">Same vocabulary</a></li>
<li><a href="#different-vocabularies">Different vocabularies</a></li>
</ul></li>
<li><a href="#validation">Validation</a></li>
<li><a href="#references">References</a></li>
</ul></div><div class="col-md-8 col-sm-12 post-content order-md-first"><p>While working on <a href="https://github.com/browsermt/bergamot-translator">bergamot-translator</a>, one feature I was tasked with implementing was a translation flow using pivoting. This post is an account of the implementation of the pivoting feature - traversing math I’m to date still not certain of, hacking one thing after another together effectively engineering a feature shipped and running in about <a href="https://addons.mozilla.org/en-US/firefox/addon/firefox-translations/">300k installations</a> now.</p>
<p>The idea is simple - given a source language <span class="math inline">\(ss\)</span> translate to a target language <span class="math inline">\(tt\)</span>, through a pivot language <span class="math inline">\(pp\)</span>. Given how history put us in an <em>anglocentric</em> position, there happens to be a lot of parallel-data between English and other languages of the world, than between other pairs. Sentence aligned parallel data aligned drives machine-translation and this consequently meant the existence of a lot of <span class="math inline">\(ss \rightarrow en\)</span> models and <span class="math inline">\(en \rightarrow tt\)</span> models. Consequently pivoting through English to obtain <span class="math inline">\(ss \rightarrow tt\)</span> remains a prevalent strategy in the field of machine-translation. At the time, the Bergamot Project stood in agreement with its model inventory.</p>
<p>The text part of pivoting is straight-forward. However, one critical-piece to keeping a user-facing HTML translation feature functional was obtaining alignments between the source and target properly. Critical, because the HTML translation feature relied on working alignment information to place tags correctly when matching-tokens in source and target moved around. HTML translation feature was ready with vanilla-translation without pivoting and using alignments. For the pivoting case, attention matrices providing alignments for source to pivot and pivot to target was available separately.</p>
<p>Using these to obtain source to target alignments wasn’t straightforward (at least to me, back then). So I’ve decided this warrants a post, albeit a bit late.</p>
<h2 id="attention">Attention</h2>
<!--
@luong2015effective formulates attention as follows:

\begin{align}
    \mathbf{a} &= \mathrm{align}(\mathbf{h}_t, \mathbf{h}_s) \\
               &= \dfrac
                                {\exp \left(\mathrm{score}\left(\mathbf{h}_t, \mathbf{h}_s\right)\right)}
                                {\sum_{s'}{\exp\left(\mathrm{score}\left(\mathbf{h}_t, \mathbf{h}_s'\right)\right)}}
\end{align}
-->
<p>Marian, the library bergamot-translator is built on top of <a href="https://github.com/marian-nmt/marian-dev/blob/c29cc83dc4d234f0e3a00a46a729053132b408b8/src/models/costs.h#L86">uses</a> an additional loss formulated using attention to additionally learn <em>alignments</em> between source and target tokens. During training, a guided alignment loss is added to the objective that uses alignment information as a training signal. A tool matching tokens in source to tokens in target from raw-corpus like <a href="https://github.com/clab/fast_align">fastalign</a> can provide alignments that can be used as ground-truths. At inference, the attention values will predict the expected alignments. Note that this is different from learning to align, the network is forced to learn to align to certain ground truths from alignment data here. More on this process can be found in <span class="citation">Chen et al. (<a href="#ref-chen2016guided">2016</a>)</span>.</p>
<p>Illustrations visualizing attention are often attached as qualitative samples demonstrating attention’s efficacy. The render below comes from something <a href="https://github.com/jerinphilip/bergamot-translator/pull/88">I repurposed</a> with borrowed source from <a href="https://distill.pub/2016/augmented-rnns/">distill.pub</a>.</p>
<div class="figure">
<img src="../static/images/pivoting-de-en.png" alt="Translating German to English: Alignments via attention" />
<p class="caption">Translating German to English: Alignments via attention</p>
</div>
<p>The above illustration also provide hints to underlying data, and data-structures. The initiated should immediately recognize the tokens (on source and target) can be modelled as nodes and the arrows modelled as weighted edges between source and target tokens.</p>
<p>In code, we have an adjacency-matrix describing the probabilities / scores matching a source-token with a target-token.</p>
<h2 id="pivoting">Pivoting</h2>
<p>As mentioned before, the idea is simple - given a source language <span class="math inline">\(ss\)</span> translate to a target language <span class="math inline">\(tt\)</span>, through a pivot language <span class="math inline">\(pp\)</span>. To get a hang of the elements involved in a translation from German to Italian via English as pivot, see illustrations below.</p>
<div class="figure">
<img src="../static/images/pivoting-de-en.png" alt="German to English" />
<p class="caption">German to English</p>
</div>
<div class="figure">
<img src="../static/images/pivoting-en-it.png" alt="English to Italian" />
<p class="caption">English to Italian</p>
</div>
<p>We have two adjacency matrices in-play here, one with scores for source-pivot pair and the other with scores for pivot-target pair. For simplicity’s sake, we’ll first try to come up with a formulation and algorithm for the case when the pivot tokens match (<em>i.e</em> same vocabulary) and matrix multiplication is straighforward.</p>
<h3 id="same-vocabulary">Same vocabulary</h3>
<p>Let the tokens involved in pivoting be <span class="math inline">\(S, P, T\)</span> denoting source, pivot and target respectively.</p>
<span class="math display">\[\begin{align*}
    S = \{s_i\} &amp;= \{s_1, s_2, \dots \} \\
    P = \{q_j\} &amp;= \{q_1, q_2, \dots \} \\
    T = \{t_k\} &amp;= \{t_1, t_2, \dots \} 
\end{align*}\]</span>
<p>These tokens do not necessarily correspond to the notion of <em>words</em>. Note that I am using <span class="math inline">\(q_j\)</span> to represent the pivot sequence <span class="math inline">\(P\)</span> so as to not confuse with probabilities used in this document, denoted by <span class="math inline">\(p(\cdot)\)</span>.</p>
<p>From alignments coming out of the decoding pipeline, we obtain a probability for each source-token <span class="math inline">\(s_i\)</span> over the target token <span class="math inline">\(t_j\)</span>. We will use <span class="math inline">\(p(s_i | t_j)\)</span> to denote this in this post. For each target-token <span class="math inline">\(t_j\)</span> we have a probability distribution spread over source-tokens <span class="math inline">\(S\)</span>.</p>
<p>We know the values <span class="math inline">\(p(s_i | q_j)\)</span>, <span class="math inline">\(p(q_j | t_k)\)</span> at inference as some form of attention/alignment from the constituent neural network. I will cook up the math below to get the required <span class="math inline">\(p(s_i | t_k)\)</span>:</p>
<span class="math display">\[\begin{align}
p(s_i | t_k)  &amp;= \sum_{j}{p(s_i, q_j| t_k)}  \label{eq:marginalize-pivot} &amp; \text{Marginalization(?)}  \\
              &amp;= \sum_{j}{{p(s_i| q_j,  t_k) \cdot p(q_i | t_k) }}        &amp; \text{Bayes rule(?)}       \\
p(s_i | t_k)  &amp;= \sum_{j}{{p(s_i | q_j)\cdot p(q_j | t_k) }}              &amp; \text{Independence(?)}
\end{align}\]</span>
<p>In an ideal case, if we assume the pivot tokens constituting the pivot sentence are same, we have a <span class="math inline">\(|S| \times |P|\)</span> matrix and a <span class="math inline">\(|P| \times |T|\)</span> matrix. The above formulation in implementation translates to a matrix multiplication, of matrices containing attention values coming out of the source to pivot and pivot to target translation processes. Not sure if the above math is sound, I’m mostly working backwards from a gut feeling that I have two attention matrices, multiplying them should give me the required probabilities.</p>
<p>This matrix-multiplication is implemented in bergamot-translator <a href="https://github.com/jerinphilip/bergamot-translator/blob/4fed75af1deb3ede67d9ade0354b87b0806a0ad3/src/translator/response.cpp#L127-L137">here</a>. The implementation makes an additional hop, due to the vocabularies being different. We will discuss this in detail next.</p>
<h3 id="different-vocabularies">Different vocabularies</h3>
<p>In reality, it’s not as simple as above. Due to historical reasons, the <span class="math inline">\(ss \rightarrow pp\)</span> and <span class="math inline">\(pp \rightarrow tt\)</span> models happen to be be using different sets of vocabularies. If we take a closer look at the diagrams above, we see <span class="math inline">\(P\)</span> and <span class="math inline">\(P'\)</span> are different. See an extract below. The tokens are space separated.</p>
<div class="sourceCode">
<pre class="sourceCode">
<code class="sourceCode">[S ] Der heutige Artikel in Wikipedia , der freie En zy klo pä die .
[P ] To day ' s article in Wikipedia , the free en cycl o pedia .
[P'] Today ' s article in Wikipedia , the free e ncy clo pedia .
[T ] L ' articolo di oggi su Wikipedia , l ' en ciclo pedia libera .</code>
</pre>
</div>
<p>The previous formulation was convenient in our application of NMT case when the vocabulary used to represent the language <span class="math inline">\(pp\)</span> is consistent giving us <span class="math inline">\(S \leftarrow P\)</span> and <span class="math inline">\(P \leftarrow T\)</span>. To obtain the probabilities in the inconsistent case, we can use the knowledge that vocabularies match at character or byte level. Both vocabularies describe the same underlying text-surface.</p>
<p>Updating the formulation to include inconsistent pivot vocabularies, we get:</p>
<span class="math display">\[\begin{align*}
    S  = \{s_i    \} &amp;= \{s_1, s_2, \dots   \}    \\
    P  = \{q_j    \} &amp;= \{q_1, q_2, \dots   \}    \\
    P' = \{q'_{j'}\} &amp;= \{q'_1, q'_2, \dots \}    \\
    T  = \{t_k    \} &amp;= \{t_1, t_2, \dots   \}    \\
\end{align*}\]</span>
<p>The old math remains valid, but requires some reinterpretation. We will start from the formulation we already have.</p>
<span class="math display">\[\begin{align}
p(s_i | t_k)  &amp;= \sum_{j}{{p(s_i | q_j)\cdot p(q_j | t_k) }}  \\
\end{align}\]</span>
<p>Both <span class="math inline">\(q'_{j'}\)</span> and <span class="math inline">\(q_j\)</span> describe a surface in the same underlying string, which overlaps to some extent. We can use this information to proportionately assign probabilities of <span class="math inline">\(q'_{j'}\)</span> to the characters, and reinterpret them in terms of <span class="math inline">\(q_j\)</span>.</p>
<span class="math display">\[\begin{align*}
    p(q_j | t_j) &amp;= \sum_{q'_{j'}}{\mathrm{overlap}(q_j, q'_{j'}) \cdot  p(q'_{j'} | t_j)} \\
    \mathrm{overlap}(q_j, q'_{j'}) &amp;= \dfrac{\lvert q_j \cap q'_{j'} \rvert}{\lvert q'_{j'} \rvert} \\
\end{align*}\]</span>
<h2 id="validation">Validation</h2>
<p>I have cooked up a lot of math, now how do I validate it? Thankfully this is grounded in a real use-case. I can try and do German to English to Italian, but the weird thing is I don’t speak/read the source and target languages. I came up with the not-so-standard but useful use-case of translating English to Estonian to English doing the round-trip.</p>
<p>The above process is textbook definition of <em>lost in translation</em>. When translating through an intermediate language, some information is lost (or added). Some corruption to the tokens happen. However, should the alignment formulations and engineering be sound the scores should correspond for the tokens surviving <em>lost in translation</em>.</p>
<p>Armed with the above, I filtered out the top-scores and printed them on the console during development. Find some output from the time of development below (click to expand).</p>
<p><details> <summary>Sample #1</summary></p>
<pre><code>&gt; The Bergamot project will add and improve client-side machine translation in a web browser.
&lt; The Bergamot project will add and improve the translation of the client-side machine into a web browser.

The The=0.955146
 Berg  Berg=0.826679
amo amo=0.995598
t t=0.975599
 project  project=0.955401
 will  will=0.912722
 add  add=0.623312
 and  and=0.941392
 improve  improve=0.710752
 translation  the=0.21632
 translation  translation=0.636088
-  of=0.396329
 machine  the=0.685785
 machine  client=0.437611
 client -=0.627738
 client side=0.621546
 machine  machine=0.720943
 in  into=0.888125
 a  a=0.951628
 web  web=0.778772
row  b=0.541725
ser row=0.273472
ser ser=0.293319
. .=0.925082
 will =0.0982262</code></pre>
<p></details></p>
<p><details> <summary>Sample #2</summary></p>
<pre><code>&gt; Unlike current cloud-based options, running directly on users’ machines empowers citizens to preserve their privacy and increases the uptake of language technologies in Europe in various sectors that require confidentiality.
&lt; Unlike current cloud-based options, working directly on user machines allows citizens to preserve their privacy and increases the adoption of language technologies in Europe in various sectors that require confidentiality.

Unlike Unlike=0.695362
 options  current=0.526343
 cloud  cloud=0.808333
based -=0.519486
based based=0.494906
 options  options=0.726565
, ,=0.953748
 running  working=0.639273
 directly  directly=0.927166
 on  on=0.712787
 users  user=0.304554
 machines  machines=0.624575
 empower  allows=0.385191
 citizens  citizens=0.503892
 to  to=0.443355
 preserve  preserve=0.797836
 their  their=0.776323
 privacy  privacy=0.942544
 and  and=0.947178
 increases  increases=0.786974
 the  the=0.646595
take  adoption=0.835365
 of  of=0.510325
 language  language=0.873498
 technologies  technologies=0.815092
 in  in=0.853544
 Europe  Europe=0.7521
 in  in=0.87099
 various  various=0.948102
 sectors  sectors=0.864905
 that  that=0.706083
 require  require=0.929663
 confidentiality  confidentiality=0.754831
. .=0.937042
 options =0.068332</code></pre>
<p></details></p>
<p><details> <summary>Sample #3</summary></p>
<pre><code>&gt; Free software integrated with an open-source web browser, such as Mozilla Firefox, will enable bottom-up adoption by non-experts, resulting in cost savings for private and public sector users who would otherwise procure translation or operate monolingually.
&lt; Free software integrated with an open source web browser, such as Mozilla Firefox, will allow the adoption from the bottom up by non-experts, resulting in cost savings for public and private sector users who would otherwise acquire translation or operate monolinguily.

Free Free=0.56728
 software  software=0.62599
 integrated  integrated=0.883837
 with  with=0.947031
 an  an=0.839987
 open  open=0.754714
source  source=0.690299
 web  web=0.729925
row  b=0.353977
ser row=0.299766
ser ser=0.309285
, ,=0.813261
 as  such=0.704488
 as  as=0.557725
 Mo  Mo=0.860583
z z=0.997161
illa illa=0.978077
 Fire  Fire=0.983289
fo fo=0.996755
x x=0.99419
, ,=0.851676
 enable  will=0.72535
 enable  allow=0.251865
 adoption  the=0.261452
 adoption  adoption=0.643473
 bottom  from=0.282512
 bottom  the=0.456695
 bottom  bottom=0.460874
up  up=0.47468
 by  by=0.623741
 non  non=0.832823
 non -=0.305707
expert expert=0.71779
 non s=0.252237
, ,=0.891577
 resulting  resulting=0.386231
 in  in=0.745363
 cost  cost=0.685577
 savings  savings=0.798135
 for  for=0.787879
 private  public=0.50413
 private  and=0.594041
 private  private=0.719888
 users  sector=0.415775
 users  users=0.790084
 who  who=0.679675
 would  would=0.659394
 otherwise  otherwise=0.823563
 procure  acquire=0.834951
 translation  translation=0.743358
 or  or=0.935836
 operate  operate=0.720217
 mono  mono=0.744872
ling ling=0.626815
ual u=0.684935
ly ily=0.676535
. .=0.69808j
ly =0.0632002</code></pre>
<p></details></p>
<p><details> <summary>Sample #4</summary></p>
<pre><code>&gt; Bergamot is a consortium coordinated by the University of Edinburgh with partners Charles University in Prague, the University of Sheffield, University of Tartu, and Mozilla.
&lt; Bergamot is a consortium coordinated by the University of Edinburgh with partners from Charles University in Prague, the University of Sheffield, the University of Tartu and Mozilla.

Berg Berg=0.994895
amo amo=0.991532
t t=0.967724
 is  is=0.888633
 a  a=0.894048
 consortium  consortium=0.868016
 coordinated  coordinated=0.773978
 by  by=0.935519
 the  the=0.738964
 University  University=0.929871
 of  of=0.946889
 Edinburgh  Edinburgh=0.936051
 with  with=0.817898
 partners  partners=0.867468
 Charles  from=0.216679
 Charles  Charles=0.731952
 University  University=0.624898
 in  in=0.875768
 Prague  Prague=0.87894
, ,=0.938541
 the  the=0.589038
 University  University=0.910463
 of  of=0.902571
 She  She=0.881999
f f=0.992118
field field=0.992631
, ,=0.842823
 University  the=0.517793
 University  University=0.835048
 of  of=0.89574
 Tar  Tar=0.918662
tu tu=0.947869
 and  and=0.821724
 Mo  Mo=0.884175
z z=0.972944
illa illa=0.993525
. .=0.926998
 University =0.0469722</code></pre>
<p></details></p>
<p>Turns out, the tokens match most of the time strong when they’re same. The implementation works as intended!</p>
<p>The implementation of the above reinterpretation for vocabulary mismatch is available <a href="https://github.com/jerinphilip/bergamot-translator/blob/4fed75af1deb3ede67d9ade0354b87b0806a0ad3/src/translator/response.cpp#L13-L97">here</a>. This code in action, through the alignments visualization pipeline I get the following render:</p>
<div class="figure">
<img src="../static/images/pivoting-de-it.png" alt="German to Italian: alignments after corrections following pivoting." />
<p class="caption">German to Italian: alignments after corrections following pivoting.</p>
</div>
<p>Arrows appear to be pointing in the right direction as well. <em>libera</em> corresponds to <em>freie</em> - this is one case where corresponding tokens have moved around to a different position in translation. The force looks strong between the nodes in the cluster forming <em>enciclopedia</em> and <em>Enzyklopädie</em>. <em>Artikel</em> and <em>articolo</em>, <em>Wikipedia</em> and <em>Wikipedia</em> looks strong as well.</p>
<p>The test-in-the wild for this piece of code is transferring links and formatting in inline-text. So if you find yourself using Mozilla Firefox’s <a href="https://addons.mozilla.org/en-US/firefox/addon/firefox-translations/">offline translation addon</a> and the links and formatting like bold/italic etc transferring from source HTML to target HTML for non-English pairs accurately, this post is an excuse for an explanation why.</p>
<h2 id="references" class="unnumbered">References</h2>
<div id="refs" class="references">
<div id="ref-chen2016guided">
<p>Wenhu Chen, Evgeny Matusov, Shahram Khadivi, and Jan-Thorsten Peter. 2016. Guided alignment training for topic-aware neural machine translation. <em>arXiv preprint arXiv:1607.01628</em>.</p>
</div>
</div></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>Notes on Cross-Entropy in autograd</title>
    <link href="http://jerinphilip.github.io/posts/cross-entropy-derivative.html" />
    <id>http://jerinphilip.github.io/posts/cross-entropy-derivative.html</id>
    <published>2023-07-22T00:00:00Z</published>
    <updated>2023-07-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="Notes on Cross-Entropy in autograd" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/cross-entropy-derivative.html" />
        <title>Notes on Cross-Entropy in autograd</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Notes on Cross-Entropy in autograd</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Jul 22, 2023</div>

         

         
            <div><a href="../tags/posts/ml.html">ml</a>, <a href="../tags/posts/autograd.html">autograd</a></div>
         
    </div>
</div>

<div class="row">
    <div class="toc col-md-4 col-sm-12 order-md-second"><h6>Outline</h6><ul>
<li><a href="#notations">Notations</a></li>
<li><a href="#cross-entropy">Cross Entropy</a><ul>
<li><a href="#derivative">Derivative</a></li>
</ul></li>
<li><a href="#matrix-and-vector-notations">Matrix and vector notations</a></li>
<li><a href="#numerical-stability">Numerical Stability</a></li>
<li><a href="#references">References</a></li>
</ul></div><div class="col-md-8 col-sm-12 post-content order-md-first"><p>The cross-entropy loss is a commonly used loss-function in neural-network training. One of the key pieces of building an autograd is writing functions that know how to do derivatives of output w.r.t inputs (See minitorch’s <a href="https://minitorch.github.io/module1/chainrule/">backward</a>). There are already a number of good articles on the web on how to derive this for cross-entropy loss, this one is just me working this out all over again and leaving a note to my future self.</p>
<h1 id="notations">Notations</h1>
<p>This article considers the scenario where there are <span class="math inline">\(d\)</span> output classes. A neural network or something generates <span class="math inline">\(d\)</span> dimensional logits at the output layer, on which the softmax activation is applied to generate probabilities.</p>
<p>In the derivations ahead, I will use the following notations at times to simplify writing. The softmax function is denoted by <span class="math inline">\(\mathbf{\sigma}(\mathbf{x}): \mathbb{R}^d \rightarrow \mathbb{R}^d\)</span> for <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d\)</span>. Softmax converts logits <span class="math inline">\(x_i\)</span> to probabilities <span class="math inline">\(p_i\)</span>.</p>
<span class="math display">\[\begin{align*}
    \mathbf{\sigma}(\mathbf{x}) &amp;= \{ p_i \}_{i=1}^{d} \\
    p_i &amp;= \dfrac{e^{x_i}}{\sum_{j=0}^{d}{e^{x_j}}}
\end{align*}\]</span>
<h1 id="cross-entropy">Cross Entropy</h1>
<p>The cross-entropy loss for multi-class classification is formulated as follows, applying a few more operations on the output of softmax:</p>
<span class="math display">\[\begin{align*}
    \mathrm{CE}(\mathbf{x}, \mathbf{y}) 
                    &amp;= - \sum_{i}^{d}{y_i \log p_i} \\
                    &amp;= -\left[ y_c \log p_c  + \sum_{ i=0, i\neq c}^{d}{y_i \log p_i} \right]\\
                    &amp; = -y_c \log p_c \\
                    &amp;= - y_c \log \left(\dfrac{e^{x_c}}{\sum_{j=0}^{d}{e^{x_j}}}\right) \\
\end{align*}\]</span>
<h2 id="derivative">Derivative</h2>
<p>The partial derivative w.r.t a component of <span class="math inline">\(\mathbf{x}\)</span> represented by <span class="math inline">\(x_k\)</span> can be computed as below. <span class="math inline">\(p_c\)</span> and <span class="math inline">\(y_c\)</span> denotes the probability predicted and the label respectively of the label (<span class="math inline">\(c\)</span>). By design, it holds <span class="math inline">\(y_c = 1\)</span> since it’s for the true label/class, but is denoted as <span class="math inline">\(y_c\)</span> for better readability in text here.</p>
<span class="math display">\[\begin{align*}
    \dfrac{\partial\mathrm{CE}(\mathbf{x}, \mathbf{y})}{\partial{x_k}} 
       &amp;= - y_c \dfrac{\partial{\log p_c }}{x_k} \\
       &amp;= - y_c \left(\dfrac{\partial{\log p_c }}{\partial p_c}\right) \left(\dfrac{\partial p_c }{ \partial x_k}\right) \\
        &amp;= - y_c \left(\dfrac{1}{p_c}\right) \left(\dfrac{\partial p_c}{\partial x_k}\right) \\
       % &amp;= - y_c \dfrac{1}{p_i} p_i \cdot (1 - p_j) \\
       % &amp;= - \sum_{i=1}^{N}{y_i \cdot (1 - p_j)} \\
    \end{align*}\]</span>
<p>Next is expanding <span class="math inline">\(\partial pc/\partial x_k\)</span>. It is worthwhile to note that <span class="math inline">\(p_c\)</span> is the output of the softmax function - so, we also happen to be computing the derivative of softmax function on vector inputs here.</p>
<p>There are two cases for the derivative, for when <span class="math inline">\(k = c\)</span> and <span class="math inline">\(k \neq c\)</span>. This is because in one case the numerator has to be considered as a variable requiring the application of the product / quotient rule for derivatives and in the other the numerator can be considered a constant.</p>
<p>For the case when <span class="math inline">\(k = c\)</span>, we have on applying the product rule:</p>
<span class="math display">\[\begin{align*}
    \dfrac{\partial p_c}{\partial x_c} &amp;= e^{x_c} \dfrac{-1}{\left(\sum_{j=0}^{d}{e^{x_j}}\right)^2} \cdot e^{x_c} + e^{x_{c}} \left(\dfrac{1}{\sum_{j=0}^{d}{e^{x_j}}}\right) \\
    &amp;= \dfrac{e^{x_c}}{\left(\sum_{j=0}^{d}{e^{x_j}}\right)} \left[ 1 - \dfrac{e^{x_c}}{\left(\sum_{j=0}^{d}{e^{x_j}}\right)}  \right] \\
    &amp;= p_c \cdot (1 - p_c) \\
\end{align*}\]</span>
<p>In the case when <span class="math inline">\(k \neq c\)</span> we obtain:</p>
<span class="math display">\[\begin{align*}
    \dfrac{\partial p_c}{\partial x_k} &amp;={e^{x_c}} \cdot \dfrac{-1}{\left(\sum_{j=0}^{d}{e^{x_j}}\right)^2} \cdot e^{x_k} \\
            &amp;= \dfrac{e^{x_c}}{\left(\sum_{j=0}^{d}{e^{x_j}}\right)} \dfrac{-e^{x_k}}{\left(\sum_{j=0}^{d}{e^{x_j}}\right)} \\
            &amp;= - p_c \cdot p_k
\end{align*}\]</span>
<p>We can consolidate the result as:</p>
<span class="math display">\[\begin{align*}
 \dfrac{\partial\mathrm{p_c}(\mathbf{x}, \mathbf{y})}{\partial{x_k}} &amp;= \begin{cases}
    p_c (1 - p_c) &amp; k = c \\
    - p_c p_k &amp; k \neq c
\end{cases}
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
 \dfrac{\partial\mathrm{CE}(\mathbf{x}, \mathbf{y})}{\partial{x_k}} &amp;= \begin{cases}
    (p_k - 1) &amp; k = c \\
    p_k &amp; k \neq c
\end{cases}
\end{align*}\]</span>
For computational convenience without branching, we can consolidate this further as:
<span class="math display">\[\begin{align*}
 \dfrac{\partial\mathrm{CE}(\mathbf{x}, \mathbf{y})}{\partial{x_k}} &amp;= (p_k - \mathbb{I}[k=c])
\end{align*}\]</span>
<h1 id="matrix-and-vector-notations">Matrix and vector notations</h1>
<p>Looking closely, one can observe that derivatives above can be written in a vector/matrix notation. The cross-entropy function is from a vector input to a scalar value (loss). <em>i.e</em>, <span class="math inline">\(\mathrm{CE}: \mathbb{R}^d \rightarrow \mathbb{R}\)</span>. Because of this, the derivative can be represented as a <span class="math inline">\(d \times 1\)</span> matrix, which is only really a vector. In literature this can be denoted by prefixing a <span class="math inline">\(\nabla\)</span>.</p>
<span class="math display">\[\begin{align*}
\nabla_{\mathbb{x}}{\mathrm{CE}} &amp;=  \left[ p_k - o_k\right]_{k=1}^{d}, &amp; o_k = \mathbb{I}[k == c]
\end{align*}\]</span>
<p>The derivative of the softmax function w.r.t its inputs form a matrix of <span class="math inline">\(d \times d\)</span>, since it is a vector valued function of a vector. <em>i.e</em>, <span class="math inline">\(\mathbb{\sigma}_{\mathbb{x}}: \mathbb{R}^d \rightarrow \mathbb{R}^d\)</span>. The matrix of partial derivatives of one output component w.r.t another input component forms the Jacobian, denoted often by <span class="math inline">\(\mathbf{J}\)</span>. <span class="math inline">\(J\)</span> doesn’t appear much pleasant to unroll here, but from the two-indices in the equation above, it should be evident this forms a matrix.</p>
<p>This notation is what you’ll find in Eli Bendersky’s <a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">post</a> and CS224n’s <a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf">Vectorized Gradients</a>, both of which I found useful. I just preferred at the time of writing this post to do things by hand using even more basic math (the one I learned in school) due to the lack of familiarity with multivariate calculus vocabulary used in places.</p>
<h1 id="numerical-stability">Numerical Stability</h1>
<p>While doing the implementation, one has to be mindful of the numerical stability. This is why some computations of the softmax function looks slightly more complicated. Take a look at marian’s <a href="https://github.com/marian-nmt/marian-dev/blob/master/src/tensors/cpu/tensor_operators.cpp#L950-L984">CrossEntropy backward</a>, which computes the probabilities from softmax for the derivative, for example.</p>
<p>The usual trick applied is to multiply the numerator and denominator in <span class="math inline">\(p_i\)</span> with a constant, making <span class="math inline">\(e^{x}\)</span> more amenable to floating point computations.</p>
<span class="math display">\[\begin{align*}
    p_i &amp;= \dfrac{e^{x_i}}{\sum_{j=0}^{d}{e^{x_j}}} 
        &amp;= \dfrac{e^{x_i}}{\sum_{j=0}^{d}{e^{x_j}}} \times \dfrac{e^{-M}}{e^{-M}} 
        &amp;= \dfrac{e^{(x_i - M)}}{\sum_{j=0}^{d}{e^{(x_j - M)}}} 
\end{align*}\]</span>
<p>If we choose <span class="math inline">\(M\)</span> as follows as the max among <span class="math inline">\(x_i\)</span>, we get less troubles of overflow and the likes:</p>
<span class="math display">\[\begin{align*}
M = \max \{ x_i \}
\end{align*}\]</span>
<p>My own reference implementation looks something like below:</p>
<div class="sourceCode"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span class="dt">void</span> cross_entropy_with_logits_grad(<span class="dt">float</span> *logits, <span class="dt">int</span> *labels,
                                    <span class="dt">size_t</span> batch_size, <span class="dt">size_t</span> num_classes,
                                    <span class="dt">float</span> *grad_out) {
  <span class="cf">for</span> (<span class="dt">size_t</span> i = <span class="dv">0</span>; i &lt; batch_size; i++) {
    <span class="dt">size_t</span> offset = i * num_classes;

    <span class="dt">float</span> *x = logits + offset;
    <span class="dt">float</span> *dce = grad_out + offset;
    <span class="kw">auto</span> label = <span class="kw">static_cast</span>&lt;<span class="dt">size_t</span>&gt;(labels[i]);

    <span class="co">// Find maximum among x-s</span>
    <span class="dt">float</span> max_value = <span class="bu">std::</span>numeric_limits&lt;<span class="dt">float</span>&gt;::lowest();
    <span class="cf">for</span> (<span class="dt">size_t</span> j = <span class="dv">0</span>; j &lt; num_classes; j++) {
      max_value = <span class="bu">std::</span>max&lt;<span class="dt">float</span>&gt;(max_value, x[j]);
    }

    <span class="co">// Find sumexp after subtracting max-value.</span>
    <span class="dt">float</span> sumexp = <span class="dv">0</span>;
    <span class="cf">for</span> (<span class="dt">size_t</span> j = <span class="dv">0</span>; j &lt; num_classes; j++) {
      sumexp += <span class="bu">std::</span>exp(x[j] - max_value);
    }

    <span class="co">// Compute gradients using predicted probability.</span>
    <span class="cf">for</span> (<span class="dt">size_t</span> j = <span class="dv">0</span>; j &lt; num_classes; j++) {
      <span class="dt">float</span> p = <span class="bu">std::</span>exp(x[j] - max_value) / sumexp;
      <span class="kw">auto</span> o = <span class="kw">static_cast</span>&lt;<span class="dt">float</span>&gt;(j == label);
      dce[j] = (p - o);
    }
  }
}
</code></pre></div>
<h1 id="references">References</h1>
<ol style="list-style-type: decimal">
<li><a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf">CS224n: Gradient Notes</a></li>
<li><a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">Eli Bendersky: The Softmax function and its derivative</a></li>
<li><a href="https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/">MLDawn: Backprop, xent and softmax</a></li>
</ol></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>Force integrated graphics</title>
    <link href="http://jerinphilip.github.io/posts/always-egpu.html" />
    <id>http://jerinphilip.github.io/posts/always-egpu.html</id>
    <published>2023-06-12T00:00:00Z</published>
    <updated>2023-06-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="Force integrated graphics" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/always-egpu.html" />
        <title>Force integrated graphics</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Force integrated graphics</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Jun 12, 2023</div>

         

         
            <div><a href="../tags/posts/arch.html">arch</a>, <a href="../tags/posts/linux.html">linux</a>, <a href="../tags/posts/graphics.html">graphics</a></div>
         
    </div>
</div>

<div class="row">
    <div class="col-md-8 col-sm-12"><p>As of now, applications which use graphics tend to use NVIDIA.</p>
<details> <summary> <code>$ nvidia-smi </code> </summary>
<pre>
Mon Jun 12 14:26:03 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3060         Off| 00000000:01:00.0  On |                  N/A |
|  0%   40C    P8               12W / 170W|   1284MiB / 12288MiB |     11%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A      1299      G   /usr/lib/Xorg                               523MiB |
|    0   N/A  N/A      1391      G   /usr/bin/gnome-shell                        108MiB |
|    0   N/A  N/A      1592      G   /usr/bin/gnome-software                      20MiB |
|    0   N/A  N/A      1817      G   /usr/lib/xdg-desktop-portal-gnome            69MiB |
|    0   N/A  N/A      2794      G   /usr/bin/kitty                                3MiB |
|    0   N/A  N/A      8396    C+G   ...4175876,13528862819532032333,262144      552MiB |
+---------------------------------------------------------------------------------------+`

</pre>
<p></details></p>
<p>I was thinking of a use-case to always allocate the GPU VRAM to Machine Learning models. This should be possible, but weird. Given recent uptick in deep-learning queries on the internet, someone or the other should have run into the same problem.</p>
<p>On looking for an adaptation of <a href="https://gist.github.com/wangruohui/bc7b9f424e3d5deb0c0b8bba990b1bc5">an ubuntu script</a> for <a href="../posts/buildapc.html">the newly installed ArchLinux</a>, I received the following pointers from the <a href="https://app.element.io/#/room/#archlinux:archlinux.org">matrix channel</a>:</p>
<ol style="list-style-type: decimal">
<li><a href="https://aur.archlinux.org/packages/hyprland-nvidia">hyprland-nvidia</a></li>
<li><a href="https://www.reddit.com/r/linux_gaming/comments/vh0f03/possible_to_use_intel_igpu_on_wayland_but_nvidia/">r/linux_gaming/…/possible_to_use_intel_igpu_on_wayland_but_nvidia</a></li>
<li><a href="https://github.com/ewagner12/all-ways-egpu">gh/ewagner12/all-ways-egpu</a></li>
</ol>
<p>I haven’t yet felt the need to do walk this path for now. This post will be updated with the details if I get to execution. For now, this will remain a link stash.</p>
<h2 id="workaround">Workaround</h2>
<p>Coincidental, but relative of mine needed more VRAM space for ML experiments (they’re all the rage it seems, I figure). Between our discussions, the following workaround is nice if you have a second machine and can use the GPU machine as a headless one without display.</p>
<p>It’s possible to reconnect to the motherboard display-port/HDMI to use the integrated graphics, and disable display. This allows NVIDIA driver to work, while display rendering does not use the GPU and the VRAM remains available.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">systemctl</span> disable gdm --now </code></pre></div>
<p>To revert, it’s always possible to use:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">systemctl</span> enable gdm --now </code></pre></div>
<p>Since I still have my ThinkPad X1C, I think I should be able to use the method if it comes to that. The above assumes GNOME is the default. It is the case for me and the person I’m corresponding with.</p></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>Building a PC</title>
    <link href="http://jerinphilip.github.io/posts/pcbuild.html" />
    <id>http://jerinphilip.github.io/posts/pcbuild.html</id>
    <published>2023-06-10T00:00:00Z</published>
    <updated>2023-06-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="Building a PC" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/pcbuild.html" />
        <title>Building a PC</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Building a PC</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Jun 10, 2023</div>

         

         
            <div><a href="../tags/posts/build.html">build</a>, <a href="../tags/posts/hardware.html">hardware</a></div>
         
    </div>
</div>

<div class="row">
    <div class="col-md-8 col-sm-12"><p>I got my first PC sometime in 2004. It had a CRT Monitor, 256MB DDR1 RAM, Pentium IV Processor. I don’t remember how much storage the machine had, but it sure was a mechanical hard-drive. I didn’t even know what a computer was back then, my dad must’ve been a visionary who got me one in my remote corner of the world. This machine was the beginning of the path that led me into programming and computer science.</p>
<p>I’ve been wanting to build my own machine a while, but been procrastinating it for long. Come to think of it, what held me back was the lack of stability over the past few years. I was graduating, moving countries, being on visa. If I had to uproot my life again quickly, the overhead of moving PC components would be an added woe.</p>
<p>2 decades and 3 computers later, I have managed to end up with some time to myself, and some stability back home. Some things were already in place. Last May when I came to India, I purchased and assembled a <a href="https://www.amazon.in/TEKAVO-computer-Computer-Workstation-Reversible/dp/B09B6DQYXH">nice table</a> that could support the build. A basic <a href="https://www.amazon.in/Featherlite-Ergonomic-Adjustable-Support-Armrest/dp/B09DNZPTJ8">featherlite chair</a> was also purchased and kept, and I have an extra from my Bangalore setup.</p>
<p>The first-laptop I got for college is still lying around in my room, waiting for some restoration. The ThinkPad X1C7 I upgraded to is what I’m writing this post from. The laptop is a super-portable machine, but it just wasn’t cutting it for certain heavy compile and some light ML training workloads I want to take on.</p>
<h2 id="build">Build</h2>
<p>The machine I want is something that supports a lot of heavy-compilations and some prototyping on the GPU. As of now, I do not want to do training workloads. For a usable model, this should require heavier GPUs and more power-draw. My short term interests are only to learn GPU/CUDA Programming using the new toy. My experience with compile jobs are <code>make -j</code> likes more workers (CPU cores) and more memory (RAM).</p>
<p>My good-friend Amaljith has been tolerating my chatterbox on a vision to build a PC maxing out every component for years now. Having built a gaming PC for himself already, I consider him an expert in the field, even more so than me. In theory I should know better because of my computer science background and sysadmin work, but I’ve been busy with other stuff. The original build had multiple 4090s planned, but for all the cheap talk I did and how I scaled it down so much, I think he reserves every right to shame for life. Our good-friend Aurobindo also offered help and some inputs.</p>
<p>Once the decision to buy was in-place and use-cases were covered where thought about, components got decided fast. The final list of components are below:</p>
<table>
<thead>
<tr class="header">
<th>Component</th>
<th>Model</th>
<th>Specs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Processor</td>
<td>AMD Ryzen 9 7950X</td>
<td>4.5GHz, 16 cores x 2 threads</td>
</tr>
<tr class="even">
<td>Motherboard</td>
<td>MSI Pro X670-P Wifi</td>
<td>X670</td>
</tr>
<tr class="odd">
<td>RAM</td>
<td>Crucial CT32G48C40U5</td>
<td>64GB (32GBx2) DDR5 4800MHz</td>
</tr>
<tr class="even">
<td>SSD</td>
<td>Kingston NVME INTERNAL SSD (SNV2S/1000G)</td>
<td>1TB NV2 M.2 2280 PCIE 4.0</td>
</tr>
<tr class="odd">
<td>GPU</td>
<td>INNO3D GeForce RTX 3060</td>
<td>12GB VRAM</td>
</tr>
<tr class="even">
<td>CPU Cabinet</td>
<td>Corsair 4000D Black</td>
<td>Mid-Tower</td>
</tr>
<tr class="odd">
<td>CPU Cooler</td>
<td>Noctua NH-D15S Chromax Black</td>
<td>Air Cooler, Silent</td>
</tr>
<tr class="even">
<td>PSU</td>
<td>EVGA SuperNOVA 1000 GT</td>
<td>1000W 80+ Gold Fully Modular</td>
</tr>
<tr class="odd">
<td>Monitor</td>
<td>Samsung LU32J590UQW</td>
<td>80.1cm (31.5“) UHD 4k QLED</td>
</tr>
<tr class="even">
<td>Keyboard</td>
<td>Anne Pro 2</td>
<td>Bluetooth 60%</td>
</tr>
<tr class="odd">
<td>Mouse</td>
<td>Logitech M185</td>
<td>Wireless</td>
</tr>
</tbody>
</table>
<p>I do not think the build is particulary fancy, I cheaped out on plenty of components. The power-supply is an overkill (1000W), but let’s hope I can sneak in a few more GPUs and other enhancements in the future. This is perhaps far-fetched, but if a mistake, this is not too costly.</p>
<p>I did not purchase a UPS. My home has a UPS Inverter + Battery already installed that powers a whole lot of devices. Back when it was being setup, I made sure the sockets in the intended office room had power delivered via the inverter. After some correspondence with the local technician who maintains the setup and expected power-draw from my machine, we concluded the inverter is enough, no need for an additional UPS.</p>
<h2 id="procurement">Procurement</h2>
<p>Edinburgh spoiled me with 4K screens, so I had already got one when I joined a new job in Bengaluru (sometime in August 2022). The Anne Pro 2 was purchased while in Edinburgh (circa October 2020). Mouse was a random purchase while in Edinburgh, don’t exactly remember when.</p>
<p>I had to buy the rest, and also ensure these got delivered to the middle-of-nowhere. I used <a href="pcpricetracker.in" class="uri">pcpricetracker.in</a> to scout cheapest vendors to procure my required components from. Most of the argmin on price on my requirements came from <a href="https://www.vedantcomputers.com/">Vedant Computers</a>. There were a few that were cheaper on other websites, but in some cases a delivery/credit-card surcharge offset it. In the end I purchased the RAM from <a href="https://www.theitdepot.com/">ITDepot</a>, PSU from <a href="www.pcstudio.in">PCStudio</a>.</p>
<p>I ordered the cabinet first to test delivery. Turns out it was sent via Delhivery and I had <a href="https://twitter.com/jerinphilip_/status/1664470235305459713">troubles getting it in time</a>. I didn’t wait, and went ahead and ordered all remaining components on June 01, 2023 - expecting to assemble as soon as possible. Delhivery held my cabinet package for nearly a week more than the expected time, at which point all components had arrived at the final-delivery hub. I somehow troubled customer care to the point they told me the (wrong) address to the final delivery center, which turned out to be a drive away. After clicking buttons on the delhiver portal I found the hub and picked up 3 packages that had arrived via Delhivery. RAM was shipped via Amazon Shipping, it took another day to arrive.</p>
<p>Around June 08, I had all required components, and was ready to assemble. I made unboxing videos. These were required in case I wanted to return and get a replacement within the stipulated time.</p>
<h2 id="assembly">Assembly</h2>
<p>In the past I have opened up more expensive server-equipment and maintained them - swapped out a few components. I have never before assembled a PC on my own. Given there were fragile components of high value, this endeavour had me worried.</p>
<p>I only watched a few component specific videos. In general, I consume text faster than videos and drawn out assembly videos are painful. I was aware I could mess-up the CPU were I not delicate from browsing forums like <a href="https://www.reddit.com/r/buildapc/">r/buildapc</a> and reading the wiki. I found the text-guides in <a href="reddit.com/r/buildapc/wiki/beginnersguide">Beginners Guide</a> particularly useful.</p>
<p>I had a sense of direction inside my head - put CPU in, plant CPU cooler on top. Insert RAM - I had done this before and is easy. Insert NVME SSD on M.2. This was new, I had to watch a few videos. If I connect this to the Front Panel controls and the power-supply I could boot into BIOS if all worked out. This meant I kept the GPU and a spare RAM aside, in case I fried the components by wrong connections. I also didn’t connect the body fans in the beginning, just the CPU Cooler Fan was connected.</p>
<p>The power-supply had asymmetric tabs on both ends. When they were symmetric - it meant both sides were compatible. This helped my fear of blowing up any parts connecting the wires wrong. It was mentioned somewhere in the forums such a component frying possibility existed.</p>
<p>I found connecting the CPU power-supply socket on the motherboard to the power-supply cables more difficult than expected. Once the motherboard was screwed in to the case, the connector was in a corner I could not reach easily. To get around this, I had to remove the motherboard and insert cable, then put it back in. Given a heavy cooler, this risked bending motherboard, I realized quickly.</p>
<p>Another hiccup along the route was that the power-supply, since it was rated 1000W came with a power-socket male plug (16A). However, all sockets in the room where the machine is intended to be kept is 6A normal plug. Since this is India and <em>jugaad</em> runs in the blood, I went to the local electrical shop and got a 6A to 16A adapter. I think this would be unavailable due to safety rules in western countries I have stayed before.</p>
<p>To kill a few more birds with one stone of a town-visit, I bought small stuff that could improve quality-of-life. These included small zip-lock bags to keep the leftover screws and that sort, one transparent box to keep all of these in, price tag stickers which I will repurpose as labels for screws.</p>
<h2 id="software">Software</h2>
<p>Software is what I’m good at.</p>
<p>There was a plan of doing some mild-gaming on this machine - it’s capable of gaming after all. However, I got fronted with an install driver window when attempting to install Windows 11. After reading a few forums and getting reminders of what kind of a debugging/fixing hell windows was, I decided to chuck Windows ideas and go only Linux.</p>
<p>I proceeded to install my favourite distro - ArchLinux. I have to do some deep-learning work. Ubuntu could’ve been a sensible choice considering support. But I felt confident enough to make do the same stuff with ArchLinux.</p>
<pre>
<span style="font-weight:bold;color:blue;">
               +                OS:</span> Arch Linux x86_64
<span style="font-weight:bold;color:blue;">               #                Hostname:</span> vty
<span style="font-weight:bold;color:blue;">              ###               Kernel Release:</span> 6.3.6-arch1-1
<span style="font-weight:bold;color:blue;">             #####              Uptime:</span> 1 day, 19:53
<span style="font-weight:bold;color:blue;">             ######             WM:</span> None
<span style="font-weight:bold;color:blue;">            ; #####;            DE:</span> GNOME
<span style="font-weight:bold;color:blue;">           +##.#####            Packages:</span> 1108
<span style="font-weight:bold;color:blue;">          +##########           RAM:</span> <span style="color:blue;"></span><span style="font-weight:bold;color:green;">8326 MB</span> / 63450 MB
<span style="font-weight:bold;color:blue;">         ######</span><span style="color:blue;">#####</span><span style="font-weight:bold;color:blue;">##;         Processor Type:</span> AMD Ryzen 9 7950X 16-Core Processor
<span style="font-weight:bold;color:blue;">        ###</span><span style="color:blue;">############</span><span style="font-weight:bold;color:blue;">+        $EDITOR:</span> vim
<span style="font-weight:bold;color:blue;">       #</span><span style="color:blue;">######   #######        </span><span style="font-weight:bold;color:blue;">Root:</span> <span style="color:blue;"></span><span style="font-weight:bold;color:green;">217G</span> / 916G (23%) (ext4)
<span style="color:blue;">     .######;     ;###;`&quot;.      
    .#######;     ;#####.       
    #########.   .########`     
   ######'           '######    
  ;####                 ####;   
  ##'                     '##   
 #'                         `#  
</span>
</pre>
<p>A four-years ago me would’ve went ahead and configured every detail, but I got no time for that much these days. Just installed GNOME on the base system, defaults are nice. I’ll switch to a tiling WM if the workflow necessitates it at some point in the future.</p>
<p>I expected GNOME would come with <code>network-manager</code>, but turned out it did not. So I had to boot into the live-disk again, setup internet, <code>arch-chroot</code> and then install <code>networkmanager</code>. I have made this mistake before, but some things you keep doing again and again.</p>
<p>I encountered a few more issues at configuring the graphics card with the proprietary driver. Turns out <code>wayland</code>, <code>sway</code> etc does not play nice with the proprietary driver. But I need the driver for CUDA programming.</p>
<h2 id="afterthoughts">Afterthoughts</h2>
<p>I see value in adding more RAM. I tried to do <code>make -j</code> on some LLVM source code and the compile OOM-ed out at some point (or so I think). Motherboard supports a maximum of 128GB. I definitely made a bad bet on storage. I am better off with a 2TB SSD, and more SATA HDDs to store lazy stuff like movies, music and that sort. I downloaded a few machine-learning models and datasets and I’m already at some 200GB (see <code>archey3</code> output somewhere above). Good thing about a PC Build is I can add more parts to upgrade. So when I have funds, and if the stay at home sticks I’ll incrementally upgrade the machine.</p>
<p>The old 2004 PC once had a lizard crawl into the power-supply and create a short-circuit. Scared the hell out of me, all the explosions back then. I thought I destroyed the entire computer gaming. The Corsair 4000D’s orifices to allow ventilation allows certain insects, so I slightly worry about possible pest attacks. One of the tradeoff of living in the middle of beautiful lush green.</p>
<p>There’s a looming concern of how much longer I’d be able to play with this new toy. At some point, I’ll have to look out for a job. From the direction the sentiment towards work from home is moving, chances are I will need to move.</p></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>Speeding up builds with ccache</title>
    <link href="http://jerinphilip.github.io/posts/ccache.html" />
    <id>http://jerinphilip.github.io/posts/ccache.html</id>
    <published>2022-03-19T00:00:00Z</published>
    <updated>2022-03-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="Speeding up builds with ccache" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/ccache.html" />
        <title>Speeding up builds with ccache</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Speeding up builds with ccache</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Mar 19, 2022</div>

         

         
            <div><a href="../tags/posts/cpp.html">cpp</a>, <a href="../tags/posts/ci.html">ci</a>, <a href="../tags/posts/github-actions.html">github-actions</a></div>
         
    </div>
</div>

<div class="row">
    <div class="toc col-md-4 col-sm-12 order-md-second"><h6>Outline</h6><ul>
<li><a href="#ccache">ccache</a></li>
<li><a href="#local-builds">Local builds</a></li>
<li><a href="#github-actions">GitHub Actions</a></li>
<li><a href="#outcome">Outcome</a></li>
</ul></div><div class="col-md-8 col-sm-12 post-content order-md-first"><p><a href="https://github.com/marian-nmt/marian-dev">marian-dev</a> has builds which takes &gt; 30mins. When I first tried to build marian-dev to edit something in sentencepiece on my personal laptop, a Lenovo ThinkPad X1 carbon - it took ages. Often I had to remove the built files and run a clean build once again. Sometimes I had to build <code>Release</code>, other times <code>Debug</code>. These days I develop on an 80-core Intel Xeon Phi, so the build times are not as much an issue. But still every now and then some noob tries to build the project on their local machine without the know-hows and often takes very very long to finish.</p>
<p>The same was the case across Windows, Linux and MacOS and cross-compilation targetting WebAssembly via emscripten when I started working for the bergamot-project - all of which had a CI build running. <a href="https://github.com/browsermt/bergamot-translator">bergamot-translator</a> uses a <a href="https://github.com/browsermt/marian-dev">fork</a> of marian-dev and the situation is pretty much the same.</p>
<div class="figure">
<img src="https://imgs.xkcd.com/comics/compiling.png" alt="My code is compiling" />
<p class="caption">My code is compiling</p>
</div>
<p>This was a point of frustration when I started, and over weekends, outside officially assigned tasks I have successfully managed to bring down the time required for each one by one.</p>
<h2 id="ccache">ccache</h2>
<p><a href="https://github.com/ccache/ccache">ccache</a> speeds up compilation by using previous compilations. The principle is quite simple - each compilation unit can be associated with <code>(source-file, compiler, compilation-args)</code>. If we hash all 3 and store the cached result somewhere, we will safely be able to reuse it in future compilations.</p>
<p>ccache, at the time of writing this post, supports most of Linux and gcc/clang, MacOS and AppleClang. It <a href="https://github.com/ccache/ccache/pull/506">recently managed support for MSVC on Windows</a>, although bergamot-translator still uses the fork with a release (We should switch soon, when I have free time). emscripten compiler (<code>emcc</code>) running on any platform <a href="https://github.com/emscripten-core/emscripten/pull/13681">has some form of support</a>. That’s pretty much all our builds - so all that’s left was to slowly add support one-by-one.</p>
<h2 id="local-builds">Local builds</h2>
<p>ccache is quite easy to set up for local builds. Chances are ccache is available in your operating system’s official package manager. The following example works with Ubuntu.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="fu">sudo</span> apt-get install ccache 
$ <span class="fu">cmake</span> <span class="va">$BUILD_DIRECTORY</span>                        \
    -DCMAKE_CXX_COMPILER_LAUNCHER=ccache        \
    -DCMAKE_C_COMPILER_LAUNCHER=ccache          \
    -DCMAKE_CUDA_COMPILER_LAUNCHER=ccache  </code></pre></div>
<p>From there-on, compilation results are going to be cached and we can rely on ccache. Most of my development happens on a Linux system, so I’m sorted. The library, however, is intended to be cross-platform (Windows, Mac, Linux, now Android). Due to Mozilla’s decisions, we also have a WebAssembly target. There’s no way I’m building everything while I local-test unless I am testing parts relevant to the platform. For that, we have GitHub CI.</p>
<h2 id="github-actions">GitHub Actions</h2>
<p>bergamot-translator uses GitHub Actions for CI. Not much documentation for GitHub actions existed when I started using it for bergamot-translator, although I found the integrated offering quite convenient. The repository was originally developed in private, but my setting up CI exhausted the private repository minutes (using the more expensive MacOS Runners) in under two days. The solution was to make the development public - no worries, it was meant to be open-sourced anyway. But we were still using more resources than necessary and adding more items to the matrix would have been difficult.</p>
<p>bergamot-translator compiles with <code>-march=native</code> for some performance reasons. This led to rather fragmented compiler flags as a function of hardware. This is not a problem when I am on my development machine and the hardware remains the same. But GitHub runners, we’ve discovered are not uniform - some have <code>avx512</code> capabilities while others have <code>avx2</code> capabilities.</p>
<p>The general skeleton on optimizing build turnaround time with ccache on GitHub actions is the same across platforms. I use the environment to store a bunch of variables, these extend to <code>$GITHUB_ENV</code>, but I’d want to reuse the variable store in a matrix as well so the structure looks like the following:</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="fu">env:</span>
  <span class="fu">ccache_basedir:</span><span class="at"> ${{ github.workspace }}</span>
  <span class="fu">ccache_dir:</span><span class="at"> </span><span class="st">&quot;${{ github.workspace }}/.ccache&quot;</span>
  <span class="fu">ccache_compilercheck:</span><span class="at"> content</span>
  <span class="fu">ccache_compress:</span><span class="at"> </span><span class="st">'true'</span>
  <span class="fu">ccache_compresslevel:</span><span class="at"> 9</span>
  <span class="fu">ccache_maxsize:</span><span class="at"> 200M</span>
  <span class="fu">ccache_cmake:</span><span class="at"> -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DCMAKE_C_COMPILER_LAUNCHER=ccache</span></code></pre></div>
<p>The place to store <code>$CCACHE_DIR</code> is GitHub and needs to sustain across builds. The following generates variables to sort the lookup by recency on the working PR.</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="kw">-</span> <span class="fu">name:</span><span class="at"> Generate ccache_vars for ccache based on machine</span>
  <span class="fu">shell:</span><span class="at"> bash</span>
  <span class="fu">id:</span><span class="at"> ccache_vars</span>
  <span class="fu">run:</span><span class="at"> |-</span>
    <span class="fu">echo &quot;:</span><span class="at">:set-output name=hash::$(echo ${{ env.ccache_compilercheck }})&quot;</span>
    <span class="fu">echo &quot;:</span><span class="at">:set-output name=timestamp::$(date '+%Y-%m-%dT%H.%M.%S')&quot;</span></code></pre></div>
<p>If the first commit, we may alternatively look into the last built <code>main</code> branch.</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="kw">-</span> <span class="fu">name:</span><span class="at"> Cache-op for build-cache through ccache</span>
  <span class="fu">uses:</span><span class="at"> actions/cache@v2</span>
  <span class="fu">with:</span>
    <span class="fu">path:</span><span class="at"> ${{ env.ccache_dir }}</span>
    <span class="fu">key:</span><span class="at"> ccache-${{ matrix.identifier }}-${{ steps.ccache_vars.outputs.hash }}-${{ github.ref }}-${{ steps.ccache_vars.outputs.timestamp }}</span>
    <span class="fu">restore-keys:</span><span class="at"> |-</span>
      ccache-$<span class="kw">{</span>{ matrix.identifier <span class="kw">}</span>}-$<span class="kw">{</span>{ steps.ccache_vars.outputs.hash <span class="kw">}</span>}-$<span class="kw">{</span>{ github.ref <span class="kw">}</span>}
      ccache-$<span class="kw">{</span>{ matrix.identifier <span class="kw">}</span>}-$<span class="kw">{</span>{ steps.ccache_vars.outputs.hash <span class="kw">}</span>}
      ccache-$<span class="kw">{</span>{ matrix.identifier <span class="kw">}</span>}</code></pre></div>
<p>The following is redundant and over-engineered, but I like to keep things this way for swappability of <code>env.var</code> and <code>matrix.var</code>.</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="kw">-</span> <span class="fu">name:</span><span class="at"> ccache environment setup</span>
  <span class="fu">run:</span><span class="at"> |-</span>
    echo <span class="st">&quot;CCACHE_COMPILER_CHECK=${{ env.ccache_compilercheck }}&quot;</span> &gt;&gt; $GITHUB_ENV
    echo <span class="st">&quot;CCACHE_BASEDIR=${{ env.ccache_basedir }}&quot;</span> &gt;&gt; $GITHUB_ENV
    echo <span class="st">&quot;CCACHE_COMPRESS=${{ env.ccache_compress }}&quot;</span> &gt;&gt; $GITHUB_ENV
    echo <span class="st">&quot;CCACHE_COMPRESSLEVEL=${{ env.ccache_compresslevel }}&quot;</span> &gt;&gt; $GITHUB_ENV
    echo <span class="st">&quot;CCACHE_DIR=${{ env.ccache_dir }}&quot;</span> &gt;&gt; $GITHUB_ENV
    echo <span class="st">&quot;CCACHE_MAXSIZE=${{ env.ccache_maxsize }}&quot;</span> &gt;&gt; $GITHUB_ENV</code></pre></div>
<p>I often leave a prolog and epolog step to diagnose over CI whether the cache is working as intended.</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="kw">-</span> <span class="fu">name:</span><span class="at"> ccache prolog</span>
  <span class="fu">run:</span><span class="at"> |-</span>
    ccache -s <span class="co"># Print current cache stats</span>
    ccache -z <span class="co"># Zero cache entry</span>

<span class="co"># Build commands go here.</span>

<span class="kw">-</span> <span class="fu">name:</span><span class="at"> ccache epilog</span>
  <span class="fu">run:</span><span class="at"> |</span>
    ccache -s <span class="co"># Print current cache stats</span></code></pre></div>
<p>With the above skeleton, turns out it is actually quite easy to set it all up. Ignoring the countless hours spent debugging how a container rolled out in a machine somewhere with feedback turnaround absurd high until the cache started working, of course. Now I get to copy-paste the above and speed up compilations across my projects.</p>
<p><strong>Linux / MacOS</strong> Linux/MacOS both worked quite out of the box with the above setup, and both had the bash shell.</p>
<p><strong>Python</strong> The python shared library via pybind11 used the gcc or clang under Linux to build, so getting this one was as simple as copying over the Linux YAML lines and adding a bunch of python keys.</p>
<p><strong>Android cross-compilation</strong> Android cross-compilation is used as “it builds” check on CI for ARM backend, which I’m pursuing at the time of writing this post. Since CMake has nice integrations as visible above, cross-compiling with a toolchain allowed me to use ccache with minimal changes required.</p>
<p><strong>Windows</strong> Windows was the odd one. Compiling things on Windows with MSVC especially has never been a fun experience. I don’t think much of the developer crowd like this either.</p>
<p>Most of the implementation followed <a href="https://cristianadam.eu/20200113/speeding-up-c-plus-plus-github-actions-using-ccache/">Speeding up C++ GitHub Actions using ccache</a>. It took some time and searching and trial and error to get it to work, and the functionality is integrated now - <a href="https://github.com/browsermt/bergamot-translator/pull/308">bergamot-translator#308</a>. Because <code>bash</code> wasn’t available, <code>cmake</code> was used to generate timestamps and such required.</p>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="kw">-</span> <span class="fu">name:</span><span class="at"> Download ccache</span>
  <span class="fu">shell:</span><span class="at"> cmake -P {0}</span>
  <span class="fu">run:</span><span class="at"> |</span>
    <span class="fu">set(ccache_url &quot;https:</span><span class="at">//github.com/cristianadam/ccache/releases/download/v${{ env.ccache_version }}/${{ runner.os }}.tar.xz&quot;)</span>
    file(DOWNLOAD <span class="st">&quot;${ccache_url}&quot;</span> ./ccache.tar.xz SHOW_PROGRESS)
    execute_process(COMMAND $<span class="kw">{</span>CMAKE_COMMAND<span class="kw">}</span> -E tar xvf ./ccache.tar.xz)
    if(ret AND NOT ret EQUAL 0)
      message( FATAL_ERROR <span class="st">&quot;Bad exit status&quot;</span>)
    endif()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="kw">-</span> <span class="fu">name:</span><span class="at"> Generate ccache_vars for ccache based on machine</span>
  <span class="fu">shell:</span><span class="at"> cmake -P {0}</span>
  <span class="fu">id:</span><span class="at"> ccache_vars</span>
  <span class="fu">run:</span><span class="at"> |-</span>
    string(TIMESTAMP current_date <span class="st">&quot;%Y-%m-%d-%H;%M;%S&quot;</span> UTC)
    <span class="fu">message(&quot;:</span><span class="at">:set-output name=timestamp::${current_date}&quot;)</span>
    <span class="fu">message(&quot;:</span><span class="at">:set-output name=hash::${{ env.ccache_compilercheck }}&quot;)</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span class="kw">-</span> <span class="fu">name:</span><span class="at"> ccache prolog</span>
  <span class="fu">run:</span><span class="at"> |-</span>
    $<span class="kw">{</span>{github.workspace<span class="kw">}</span>}\ccache.exe -sv <span class="co"># Print current cache stats</span>
    $<span class="kw">{</span>{github.workspace<span class="kw">}</span>}\ccache.exe -z <span class="co"># Print current cache stats</span>

<span class="co"># Insert build command here.</span>

<span class="kw">-</span> <span class="fu">name:</span><span class="at"> ccache epilog</span>
  <span class="fu">run:</span><span class="at"> |-</span>
    $<span class="kw">{</span>{github.workspace<span class="kw">}</span>}\ccache.exe -sv <span class="co"># Print current cache stats</span></code></pre></div>
<p>Some MSVC flags like <code>/Zi</code> where unfriendly to cache, so had to get rid of that (it was debug information, most likely).</p>
<p>Few dependencies (<code>pcre2</code>, <code>protobuf</code>) comes via <code>vcpkg</code> and are slower than what I’d want at the moment. We will look into speeding this up eventually.</p>
<p><strong>emscripten</strong> The emscripten ccache mostly referred to <a href="https://github.com/pyodide/pyodide/pull/1805">pyiodide implementation</a>. Weird flex, but <code>emcc</code> uses <code>ccache</code> compiled onto WebAssembly target and then uses it further in compilation. Since WebAssembly is intended to be a portable target - I made a choice the ccache builds cached.</p>
<p><strong>Further optimizations</strong> Originally marian-dev provided builds with debug info (<code>-DCMAKE_BUILD_TYPE=RelWithDebInfo</code>), which was inherited by bergamot-translator. This meant the compiled units had information on which lines which instructions correspond to and the information increases the size on the disk. Larger object files meant longer to compile and also getting into trouble with GitHub’s free limits.</p>
<h2 id="outcome">Outcome</h2>
<p>Compilation turnaround times we reduced as follows (in minutes):</p>
<ol style="list-style-type: decimal">
<li>Linux: 25m ➔ 5m</li>
<li>MacOS: 30m ➔ 6m</li>
<li>WebAssembly: 15m ➔ 5m (2m if optimized further)</li>
<li>Python: 30m ➔ 6m</li>
<li>Windows: 30m ➔ 10m (depending on <code>vcpkg</code> being nice).</li>
</ol>
<div class="figure">
<img src="https://imgs.xkcd.com/comics/the_general_problem.png" alt="It has indeed saved time in the long run." />
<p class="caption">It has indeed saved time in the long run.</p>
</div>
<p>Good enough to be picked up by downstream repositories as well, turns out: <a href="https://github.com/XapaJIaMnu/translateLocally/commit/a4e3e3b40e1baf955198763e99480b62495cde16">XapaJIaMnu/translateLocally@a4e3e3b</a>.</p>
<p>While this has served to reduce the compute footprint, turnaround time for developers, the ability gained by ccache has also encouraged me to add more builds - most certainly an instance of <a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons Paradox</a>.</p>
<p>Cache Invalidation is a potential problem. If at some point in the future some bug corrupts a cache entry, builds can fail. The assumption is that this does not happen often, even if it does, we can just edit a flag to recache and then the builds will go back to work.</p>
<p>Functional ccache builds for all these can be found in <a href="https://github.com/browsermt/bergamot-translator">browsermt/bergamot-translator</a>.</p></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>Lemonade IME</title>
    <link href="http://jerinphilip.github.io/posts/lemonade.html" />
    <id>http://jerinphilip.github.io/posts/lemonade.html</id>
    <published>2022-03-12T00:00:00Z</published>
    <updated>2022-03-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="Lemonade IME" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/lemonade.html" />
        <title>Lemonade IME</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Lemonade IME</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Mar 12, 2022</div>

         

         
            <div><a href="../tags/posts/mt.html">mt</a>, <a href="../tags/posts/ui.html">ui</a>, <a href="../tags/posts/linux.html">linux</a>, <a href="../tags/posts/ime.html">ime</a>, <a href="../tags/posts/cpp.html">cpp</a>, <a href="../tags/posts/bergamot.html">bergamot</a></div>
         
    </div>
</div>

<div class="row">
    <div class="toc col-md-4 col-sm-12 order-md-second"><h6>Outline</h6><ul>
<li><a href="#lemonade-ime">Lemonade IME</a><ul>
<li><a href="#verifiying-machine-generated-translations">Verifiying machine-generated translations</a></li>
<li><a href="#screencasts">Screencasts</a></li>
</ul></li>
<li><a href="#what-next">What next?</a><ul>
<li><a href="#uiux">UI/UX</a></li>
<li><a href="#development">Development</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul></div><div class="col-md-8 col-sm-12 post-content order-md-first"><p><small> This post is an account of the implementation of an Input Method Engine (IME) for translation through Intelligent Input Bus (iBus) - a software available on modern Linux desktop environments. </small></p>
<p>Most of my work in the last years have been in the vicinity of building a library for on-device machine translation. In one exchange over GitHub, I was tasked with implementing the underlying library requirements for <em><a href="https://docs.google.com/presentation/d/15ah2n58GfbP8B97nUrfl9HdKSP3Vh71-W6vZNr37eOk/">Outbound Translation</a></em> . In essence, it refers to the concept of user enter in a source language which the user knows and a software layer intercepts it to provide the target language content.</p>
<p>This problem immediately resonated with me. At some point during my internship at NAVER LABS Europe when I stayed in France, I had to contact La Poste customer care. Calling them up was out of the question - because my processing power for French audio over phone was worse. Somehow I managed to find this accessibility chat interface where I could “chat” with customer support, using text. What bothered me as a user was that the medium is French and I’m sitting with multiple tabs of Google Translate manually copying and translating content I receive, then translating the content I want to send out from English to French.</p>
<p>I have been typing my mother tongue - <em>Malayalam</em>, which uses a non-Latin alphabet and has a script of it’s own using an English (US) keyboard since high school. The technology which enabled me to do this at the time was <a href="https://swanalekha.smc.org.in/">Swanalekha</a>. Learning the <a href="https://en.wikipedia.org/wiki/InScript_keyboard">InScript Keyboard</a> looked like a hard effort, and just typing out weird combinations of the alphabet to create the intended alphabet in Malayalam felt like an easier thing to learn. The technology is enabled by iBus - which one can find today in modern Linux based operating systems with <a href="https://help.gnome.org/misc/release-notes/3.6/i18n-ibus.html.en">tight integration to the GNOME Desktop</a> - and enjoys widespread use in inputting non-Latin script by means of a Latin keyboard layout (e.g: English (US)) while providing software layers for non-Latin keyboards.</p>
<p>In a 1:1 with <a href="https://kheafield.com/">Kenneth Heafield</a>, I bring up the idea of the alternative of the entire outbound translation concept from browser-only to system-wide using the Intelligent Input Bus (iBus). While the know-hows of connecting iBus to the library was not straightforward, the possibility of the solution using both was acknowledged. However, Bergamot Project was more focused on attempting cross-platform and in the browser (on operating systems more than linux), so a pitch for a keyboard layer in the operating system (Linux specific) turned out to be a downside.</p>
<p>My counterparties at Mozilla collosally delayed the extension’s implementation of Outbound translation, leaving me plenty idle time - some of which could be redirected towards this idea. Since the tools were decided, the solution was not all that complicated. Lemonade iwould implemented in C++ as an engine that implements the <a href="https://ibus.github.io/docs/ibus-1.5/index.html">iBus interface</a>, that connects to the <a href="https://github.com/browsermt/bergamot-translator">bergamot-translator</a> C++ library. Together with the ecosystem (models, fast-nmt engine) built by the Bergamot project, lemonade manages to run the translations completely locally, providing the privacy benefits intended to be achieved by the Bergamot Project. For purposes of building an IME, I found <a href="https://github.com/epico/ibus-libzhuyin">ibus-libzhuyin</a> which I could modify and connect to the Bergamot C++ library to reach a minimum viable product. Non traditional applications like <a href="https://mike-fabian.github.io/ibus-typing-booster/">ibus-typing-booster</a> - further improved my confidence in using iBus.</p>
<h2 id="lemonade-ime">Lemonade IME</h2>
<p>The user interacts with iBus through two elements - (1) a panel available system-wide and (2) an input UI in the vicinity of the text area the user intends to input the translated text.</p>
<p><strong>Panel</strong> The panel, often available in the top-right corner for GNOME allows to choose from available input methods.</p>
<div class="figure">
<img src="../static/images/bergamot/lemonade-activated-dropdown.png">
<p class="caption">
Lemonade as an iBus engine
</p>
</div>
<p>It also allows the user to switch source language, target language and allows to configure a verify option. For now, <code>xx-&gt;xx</code> is configured to be a passthrough.</p>
<div class="figure">
<img src="../static/images/bergamot/lemonade-source-lang-selection.png" width="45%"> <img src="../static/images/bergamot/lemonade-target-lang-selection.png" width="45%">
<p class="caption">
Language selection
</p>
</div>
<p><strong>Input UI</strong> The input UI inverts the traditional usage. The existing implementation shows translation as pre-edit text, which is underlined text that is almost entered into the target text area. A commit action by the user inserts the pre-edit text into the text area. The candidate list is used to show the text that is entered in by the user before committing.</p>
<div class="figure">
<img src="../static/images/bergamot/libreoffice-without-verify.png" width="100%">
<p class="caption">
Using Lemonade IME in LibreOffice
</p>
</div>
<p>While we only have <code>xx-&gt;en</code> and <code>en-&gt;yy</code> models internally, pivoting feature allows for entering <code>xx-&gt;yy</code> by using the models involving English in sequence.</p>
<h3 id="verifiying-machine-generated-translations">Verifiying machine-generated translations</h3>
<p>Okay, now that the user is potentially trusting a machine-learning system to intercept and translate the content being put in. <em>Lost in translation</em> is a thing, sometimes with dangerous consequences - <a href="https://www.theguardian.com/technology/2017/oct/24/facebook-palestine-israel-translates-good-morning-attack-them-arrest">including getting detained</a>. How does one boost the users’ confidence in the translated text?</p>
<p><span class="citation">Zouhar et al. (<a href="#ref-zouhar2021backtranslation">2021</a>)</span> studied this as part of the Bergamot Project and came up with the UI recommendation of providing the backtranslated content additionally to the user. In this setting, we take the translated text and try to translate it back to the source language, which the user understands. If the backtranslated text match, we can be more confident that the text sent in is correct.</p>
<div class="figure">
<img src="../static/images/bergamot/lemonade-verify-backtranslation.png" alt="UI controls for verify" />
<p class="caption">UI controls for verify</p>
</div>
<!--
<div class="figure">
<img src="/static/images/bergamot/libreoffice-with-verify.png" width=100%>
<p class="caption">LibreOffice with verification</p>
</div>

<div class="figure">
<img src="/static/images/bergamot/deutche-post-form-firefox.png"  width=100%>
<p class="caption"></p>
</div>
-->
<h3 id="screencasts">Screencasts</h3>
<p>In the below screencast, I use lemonade (system-level) for outbound translation in conjunction with a local-translation browser extension - <a href="https://github.com/jelmervdl/firefox-translations">jelmervdl/firefox-translations</a>.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/JMY4ANSAKPU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<p>Lemonade also works on other applications, pretty much any text-area by intercepting the keyboard and input method. See different controls being configured on a word-processor.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Lsmpx_A_7Y8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe>
<h2 id="what-next">What next?</h2>
<p>I am grateful to Kenneth and the UI group in Bergamot to have received attention and feedback for what is a hobby horse side-project. Feedback I have received so-far notes that this is particularly useful in a chat setting, but limited when richer editing requirements are involved.</p>
<h3 id="uiux">UI/UX</h3>
<p>Since I managed a few interactions with the UI (Research) team, a lot of the problems highlighted I perceived to be quite hard.</p>
<p><strong>UI Limitations</strong> The inability to control the UI elements (primarily because of my lack of understanding of iBus) impede complex UI mechanisms. For example, once the pre-edit text is committed, there is no way to backtrack it to the original source text that the user input. This was deemed to be useful when we want complex editing workflows that are quite common on the web. This is however a constraint due to sticking to the iBus specified interface. I am keeping keeping open a <a href="https://github.com/jerinphilip/lemonade/issues/56">web-based</a> input method to gain more development capabilities.</p>
<p><strong>Flickering</strong> There is an increased instability in the text during translations. This is a necessary evil, as translations are not monotonic in nature and larger contexts lead to drastically different word orderings in the translated text. The modified translation is perhaps more suitable than an incremental one. This is a problem shared by interactive translation research and some speech-translation which requires stability as transcribed speech translations progress.</p>
<p><strong>Edit workflows</strong> Queries often arose about being able to edit/verify at word levels rather than the sentence levels after an initial draft translation was committed in. Word level editing appears to be quite hard, especially when the signals we have during inference are faint.</p>
<div class="figure">
<img src="https://imgs.xkcd.com/comics/tasks.png" alt="xkcd: Tasks" />
<p class="caption">xkcd: Tasks</p>
</div>
<h3 id="development">Development</h3>
<p>In the current setting, the implementation uses hardcoded file paths and is reliant on the bergamot python package to fetch and inventory the models. A better position is to eventually connect to <a href="https://github.com/XapaJIaMnu/translateLocally">translateLocally</a>. translateLocally’s vision is perhaps as a cross-platform GUI application - and I’m trying to convince the authors to separate out the application library out so lemonade can pick it up. The pursuit of a <a href="https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Native_messaging">Native Messaging</a> extension in translateLocally brings the features closer to the requirement of lemonade.</p>
<p>This could also be done using the Python Interface iBus allows for, but at the time Python package for bergamot was not very mature.</p>
<p>Lemonade source currently sits in <a href="https://github.com/jerinphilip/lemonade">jerinphilip/lemonade</a> under a permissive license. It has a lot of rough edges, which are expected to be smoothened over free time. If you’d like to help out with development, please feel free to drop by the GitHub issues/discussions or even contact me via email.</p>
<h2 id="references" class="unnumbered">References</h2>
<div id="refs" class="references">
<div id="ref-zouhar2021backtranslation">
<p>Vilém Zouhar, Michal Novák, Matúš Žilinec, Ondřej Bojar, Mateo Obregón, Robin L. Hill, Frédéric Blain, Marina Fomicheva, Lucia Specia, and Lisa Yankovskaya. 2021. Backtranslation feedback improves user confidence in MT, not quality. In <em>Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies</em>, pages 151–161, Online, June. Association for Computational Linguistics.</p>
</div>
</div></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>Remote work: Handling connection drops and disruptions</title>
    <link href="http://jerinphilip.github.io/posts/remote-work-handling-disruptions.html" />
    <id>http://jerinphilip.github.io/posts/remote-work-handling-disruptions.html</id>
    <published>2020-08-28T00:00:00Z</published>
    <updated>2020-08-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="Remote work: Handling connection drops and disruptions" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/remote-work-handling-disruptions.html" />
        <title>Remote work: Handling connection drops and disruptions</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Remote work: Handling connection drops and disruptions</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Aug 28, 2020</div>

         

         
            <div><a href="../tags/posts/workflow.html">workflow</a>, <a href="../tags/posts/hpc.html">hpc</a>, <a href="../tags/posts/remote-work.html">remote-work</a></div>
         
    </div>
</div>

<div class="row">
    <div class="toc col-md-4 col-sm-12 order-md-second"><h6>Outline</h6><ul>
<li><a href="#try-to-find-a-decent-nix-shell.">Try to find a decent *nix shell.</a></li>
<li><a href="#always-leave-a-session-on-the-server-tmux.">Always leave a session on the server: tmux.</a><ul>
<li><a href="#installing-tmux-locally.">Installing tmux locally.</a></li>
<li><a href="#configuring-tmux.">Configuring tmux.</a></li>
<li><a href="#auto-attach-sessions">auto-attach sessions!</a></li>
</ul></li>
<li><a href="#the-mobile-shell">The mobile shell</a><ul>
<li><a href="#configuring-own-install.">Configuring own install.</a></li>
</ul></li>
<li><a href="#local-edits-are-always-more-convenient.">Local edits are always more convenient.</a><ul>
<li><a href="#bringing-git-into-the-workflow">Bringing git into the workflow…</a></li>
</ul></li>
<li><a href="#on-phone-juicessh">On Phone: JuiceSSH</a></li>
<li><a href="#sharing-text">Sharing Text</a><ul>
<li><a href="#pastebin-services">Pastebin services</a></li>
</ul></li>
<li><a href="#epilogue">Epilogue</a></li>
</ul></div><div class="col-md-8 col-sm-12 post-content order-md-first"><p>My home is perhaps not located in the most urbanized of places. We had broadband a while back but very limited offerings. The only provider that existed - BSNL provided were absurdly slow and not value for money. At the time Jio came up as an alternative, I had asked BSNL office if they’d ever have any better offerings. They responded negative for the near future. The broadband-connection had been scrapped since then, and I had switched the home connection completely to 4G after some speedtest benchmarks. Something I have to look forward to is Fiber to the Home (FTTH) technology has presently made ways to where my home is and I might just get a good enough connection soon. I might have to bear with a little delay though. Thus fast forward a year or two from my earlier change of internet back home - I am back in a situation where I need a stronger connection, or manage with Jio amidst connection fluctuations etc etc.</p>
<p>On a recent conversation with a colleague at IIIT Hyderabad who started with MobaXterm on Windows with vanilla SSH, I realized many people don’t know these methods. This would be the umpteenth time someone has brought these troubles in front of me, and I’d rather have a document which I can point to going ahead than having to describe each time. On another thread, I have realized Sukanta Sen, soon to be work-colleague and Prof. Kenneth Heafield, my new boss have also shared some troubles with working remotely connecting to servers. They have suggested mosh, which I have already been using for about 2-3 years now and will happily start using in the new environment soon.</p>
<p>With COVID-19 lockdown situation and being stuck to working from home often experiencing interruptions with the usual providers, this is perhaps a good time time to write-up what I have been doing for a while. There are several tools which I used in the past to work around a horrible network connection. This post is specific to to maintain SSH-like sessions resilient to network drops and to avoid disruptions to the an overall programming workflow.</p>
<h1 id="try-to-find-a-decent-nix-shell.">Try to find a decent *nix shell.</h1>
<p>I haven’t historically liked the alternative offerings of PuTTY, MobaXterm, Cygwin etc. I would not thus recommend these, but I guess in the end it’s a matter of taste. Normally, working with servers have been easier for me in Linux. Since I got my new laptop and saw Windows Subsystem for Linux as a decent alternative, I have since shifted. I have an Ubuntu 18.04 currently running within my Windows OS. The following should be decent starting points to replicate my recommendation of an environment with a Windows machine.</p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Install WSL 2.0</a></li>
<li>Here’s <a href="https://www.microsoft.com/en-us/p/ubuntu-1804-lts/9n9tngvndl3q">Ubuntu 18.04 LTS</a>. Another variant if you need can be installed from even the Windows Store.</li>
</ul>
<p>On this *nix setup and one such on the remote, the content ahead describes how to get the following tooling working:</p>
<ul>
<li>tmux: installing, auto-attaching/save-resume sessions</li>
<li>mosh: building connection drop resilience and better response in UX.</li>
<li>JuiceSSH: take occassional checks when you are travelling.</li>
</ul>
<h1 id="always-leave-a-session-on-the-server-tmux.">Always leave a session on the server: tmux.</h1>
<p>I always prefer to leave tmux sessions running on a server that is guaranteed to have network connectivity. At IIIT-Hyderabad, my former university and NAVER LABS Europe where I recently finished my internship, this had been the gateway nodes which allow for such operation. In IIIT - this was atom or ada, the headnodes to the HPC clusters I worked with.</p>
<p>If you have tmux (one of the recent versions) installed already on this server, life-becomes really easy. Often server-admins prefer ancient CentOS and tmux versions, which I do not find to my taste. For the uninitiated, it’s actually really easy to install a more recent version of tmux by yourself without root-permissions. The only dependency tmux has is perhaps libevent.</p>
<h2 id="installing-tmux-locally.">Installing tmux locally.</h2>
<p>The following is subject to change, as the source of the following packages change. At the time this post is being written:</p>
<p>You can find a recent version of tmux at:</p>
<ul>
<li><a href="https://github.com/tmux/tmux/releases" class="uri">https://github.com/tmux/tmux/releases</a></li>
</ul>
<p>How you install any package locally is usually very simple, once you know the linux install routine.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">./configure</span> --prefix=<span class="va">$HOME</span>/.local/
<span class="fu">make</span> <span class="kw">&amp;&amp;</span> <span class="fu">make</span> install</code></pre></div>
<p>I have not checked this, but usually, the routine throws up a complaint when you try to do the above alone about a libevent dependency. Normally this is taken care of a package manager, but if you’re not root you’d have some additional trouble. Thankfully, tmux has in the past installed for me with just one additional dependency - which is libevent. Releases can be found somewhere below.</p>
<ul>
<li><a href="https://github.com/libevent/libevent/releases" class="uri">https://github.com/libevent/libevent/releases</a></li>
<li><a href="https://libevent.org/" class="uri">https://libevent.org/</a></li>
</ul>
<h2 id="configuring-tmux.">Configuring tmux.</h2>
<p>There are a lot of ninja-techniques to configure as well. I often try to stick with the bare minimum and defaults (not changing modifiers etc), so I can get started on the next server with minimum configuration change requirements.</p>
<p>You can find several good enough tutorials to configure tmux online.</p>
<h2 id="auto-attach-sessions">auto-attach sessions!</h2>
<p>I change machines between my laptop, desktop and phone while connecting to the servers described above. A desirable feature is consistency among all three, and saved sessions with auto-attach at the point of connection. I normally use only one tmux session and multiplex using panes or splits.</p>
<p>I have the following script which auto-attaches a tmux if it is already running, otherwise creates one for me. Add the snippet in your <code>~/.bashrc</code> or <code>~/.bash_profile</code> on the server.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co"># HOSTNAME checks and launches tmux only on the ada head-node.</span>
<span class="co"># Checks if already within a tmux as well.</span>
<span class="kw">if</span><span class="bu"> [</span> <span class="va">$HOSTNAME</span> <span class="ot">=</span> <span class="st">&quot;ada.iiit.ac.in&quot;</span><span class="bu"> ]</span> <span class="kw">&amp;&amp;</span><span class="bu"> [</span> <span class="ot">-z</span> <span class="st">&quot;</span><span class="va">$TMUX</span><span class="st">&quot;</span><span class="bu"> ]</span> ; <span class="kw">then</span>
    <span class="co"># attach if running    || launch new</span>
    <span class="ex">tmux</span> attach -dt <span class="st">&quot;main&quot;</span> <span class="kw">||</span> <span class="ex">tmux</span> new -s <span class="st">&quot;main&quot;</span>
<span class="kw">fi</span></code></pre></div>
<p>The above snippet is specific to <code>ada.iiit.ac.in</code>, but you can come up with something similar based on your setup.</p>
<p>If you’re not using bash, which is default on most systems, you should be knowledgeble enough to find the runtime-configuration file where this goes. The <code>if</code> guard is so that when <code>~/.bashrc</code> is run again on a compute node, it doesn’t create tmux-ception.</p>
<h1 id="the-mobile-shell">The mobile shell</h1>
<p>Now comes the next major problem. <em>SSH</em>. The problem with SSH is that if it disconnects it won’t reconnect back automatically. It simply hangs. Also, when you SSH, the shell waits for response to come back to echo the result of what you entered. The slowness here is extremely disruptive while developing or running something on the server. To overcome some of this and build resilience to connection disruptions we will discard SSH in favour of <a href="https://mosh.org/">Mosh</a> or the Mobile Shell.</p>
<p>The thing is, you need mosh installed at the server. On atom/cosmos, I had been root - so I can install mosh on every node I want with one command and ansible. On ada, I have enough pull to get the HPC admin to install it for me. At NAVER, this was not the case - and I installed a local version of mosh myself and configured it. A rough procedure to accomplish something similar is described below.</p>
<h2 id="configuring-own-install.">Configuring own install.</h2>
<p>I am not going to go in detail, but the same <code>configure</code>, <code>make</code> and <code>make-install</code> routine can get mosh in your local folder. But by default, mosh doesn’t look for a local mosh installation or doesn’t assume the path where it is installed. In this case</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="co"># fill remote home below.</span>
<span class="co"># In case of ada - this would be /home/$USER/.local</span>
<span class="va">HOME=</span>... 

<span class="co"># Launch mosh specifying location of where the mosh-server is installed.</span>
<span class="ex">mosh</span> --server=<span class="va">$HOME</span>/.local/bin/mosh-server <span class="op">&lt;</span>user<span class="op">&gt;</span>@<span class="op">&lt;</span>server-host<span class="op">&gt;</span></code></pre></div>
<h1 id="local-edits-are-always-more-convenient.">Local edits are always more convenient.</h1>
<p>You might think with all the above in space, you are really good to go. While mosh gives you local-echo and connection drop resilience, it still disconnects and disrupts your workflow. Have a look at <a href="https://heeris.id.au/trinkets/ProgrammerInterrupted.png">this comic</a>, which is relatable in the context.</p>
<p><a class="btn btn-secondary" data-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample"> Why you shouldn’t interrupt a programmer. </a></p>
<div id="collapseExample" class="collapse">
<div class="card card-body">
<p><img src="https://heeris.id.au/trinkets/ProgrammerInterrupted.png" alt="Why you shouldn't interrupt a programmer?"></p>
</div>
</div>
<p>
</p>
<p>The network part is still a blocking-bottleneck and a point of failure. I believe it’s fairly obvious that it is best to make edits and run locally. However, my personal machine which I optimized for portability barely has the power to run the demanding jobs I run. Editing the text source files which are run-eventually, any tiny machine can - even your android phone!</p>
<p>How I manage the changes consistently across different servers, and my personal machine which I use for edits is git. Any version control system with a central remote you can pull to make everything consistent works.</p>
<h2 id="bringing-git-into-the-workflow">Bringing git into the workflow…</h2>
<p>To enhance the workflow one more level further, I would recommend editing code through a git enabled repository which can sync at the server through a simple <code>git-pull</code>. This ensures your edits are visible as you make them and only running the command is what is dependent on the server.</p>
<ol style="list-style-type: decimal">
<li>Create an edit branch and sneak <code>git-pull</code>s into a build/run file at the server.</li>
<li>The point where you get a working version of the source, merge the branch as one commit to the main code to avoid polluting commit history.</li>
</ol>
<h1 id="on-phone-juicessh">On Phone: JuiceSSH</h1>
<p>One might not always have a laptop with Windows 10 and WSL, or Ubuntu running while travelling. This was the case at times when I was volunteering as CVIT sys-admin, and while I had been travelling in France. Sadly, there is no mosh for mobile, but I have find JuiceSSH to be decent enough for quick tasks. If you enable the auto-attaching tmux sessions, JuiceSSH can be used with minimum disruptions to occassionally check on your jobs and maybe edit if necessary to tiny degrees.JuiceSSH is available for install on the Google Play Store.</p>
<ul>
<li><a href="https://play.google.com/store/apps/details?id=com.sonelli.juicessh&amp;hl=en_IN">Google Play Store: JuiceSSH</a></li>
</ul>
<p>There are other alternatives as well, but JuiceSSH is what I like. Soon, I think I might even end up paying for the premium features the developer provides.</p>
<h1 id="sharing-text">Sharing Text</h1>
<p>Often, I end up in a situation where I have to share text generated in these servers to another place (maybe my local machine, maybe to share results with a colleague). The issue with the above setup is that often, these outputs can’t be copy-pasted using mouse (as the terminal-mosh-tmux-server intermediates will lose the clipboard).</p>
<h2 id="pastebin-services">Pastebin services</h2>
<p>What I rely on is a command-line paste-service. I used to use <a href="http://ptpb.pw">ptpb.pw</a>. However, this service has gone down since then due to abuse. Of late, I have stuck to <a href="http://ix.io">ix.io</a>. It even offers a user through <code>.netrc</code> based login, which is not very safe but I have been a happy user of. I have found this service satisfactory enough for now. You can substitute for the same with any paste-provider with a command line service. I will point to the possibilities opened by such a service and leave for you to google for the rest.</p>
<h1 id="epilogue">Epilogue</h1>
<p>This post was drafted on a TGV with the onboard WiFi, as I’m not sure how otherwise to kill time on a 4.5 hour trip to Paris from Toulouse. I have wrapped up my 6-month internship at NAVER LABS Europe, stayed a month with Phani while attempting a UK Visa, which has unfortunately failed. From Paris, I’m hoping I can take the Vande Bharat (CDG - DEL - COK), to arrive home in about 48 hours of travel.</p>
<p>Forgive any typos or errors, and please be kind-enough to point them out so I can correct if they exist. If you know better methods to stay connected and edit code, run them on servers, please do let me know in comments below.</p></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>Unsupervised Sentence Tokenization for Indian Languages</title>
    <link href="http://jerinphilip.github.io/posts/sentence-splitters-for-ilmt.html" />
    <id>http://jerinphilip.github.io/posts/sentence-splitters-for-ilmt.html</id>
    <published>2020-08-20T00:00:00Z</published>
    <updated>2020-08-20T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="Unsupervised Sentence Tokenization for Indian Languages" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/sentence-splitters-for-ilmt.html" />
        <title>Unsupervised Sentence Tokenization for Indian Languages</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>Unsupervised Sentence Tokenization for Indian Languages</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>Aug 20, 2020</div>

         

         
            <div><a href="../tags/posts/nlp.html">nlp</a>, <a href="../tags/posts/indian-languages.html">indian-languages</a>, <a href="../tags/posts/mt.html">mt</a></div>
         
    </div>
</div>

<div class="row">
    <div class="toc col-md-4 col-sm-12 order-md-second"><h6>Outline</h6><ul>
<li><a href="#premise">Premise</a></li>
<li><a href="#rules-vs-learning-from-context">Rules vs Learning from Context</a></li>
<li><a href="#enter-the-punkttokenizer">Enter the PunktTokenizer</a></li>
<li><a href="#hacking-punkttokenizer">Hacking PunktTokenizer</a><ul>
<li><a href="#getting-training-to-work">Getting training to work</a></li>
<li><a href="#injecting-custom-delimiters">Injecting custom delimiters</a><ul>
<li><a href="#process">Process</a></li>
<li><a href="#final-solutionworkaround">Final Solution/Workaround</a></li>
</ul></li>
</ul></li>
<li><a href="#merging-into-ilmulti">Merging into ilmulti</a></li>
<li><a href="#afterthoughts">Afterthoughts</a></li>
<li><a href="#references">References</a></li>
</ul></div><div class="col-md-8 col-sm-12 post-content order-md-first"><p>I had originally built the tooling around my static site generator to actually write a log of <a href="http://preon.iiit.ac.in/~jerin/">my work in CVIT</a>. Much of the content I wrote used to go there, but since I’m out now and free, I have new resolution to convert pieces that won’t make to papers here.</p>
<p>A lot of my work used to involve playing catch-up with what has already been well-implemented and working for western languages adapting them to Indian Languages. Much of these are worth penning-down, but won’t be strong enough to make to any academic publishing venues. Neither do I have the bandwidth to take it to a full-open-source well-received library now. I will hope pieces like these help out someone foraying into the area in the future.</p>
<h2 id="premise">Premise</h2>
<p>The <a href="http://pib.gov.in">Press Information Bureau</a> (PIB) articles are currently a source of training data for us to try out ideas in NMT models. A very low level yet largely critical issue which I have in my Indian Languages Machine Translation pipeline is <strong>sentence-splitting</strong>.</p>
<p>This is a point of increased irritation for me while inspecting the outputs and alignments in a web-interface I hacked up together for PIB debugging. I have realized the gravity of this problem only recently, after using crude-rule based setups to do sentence-tokenization brought about artifacts. Some of the errors in this pipeline seems to have been mitigated by some dark-magic in BLEUAlign <span class="citation">(Sennrich and Volk, <a href="#ref-sennrich2011iterative">2011</a>)</span> which works in our favour compensating any serious damage.</p>
<p>If given an option, I would just write or show code - no documentation, no non-technical aspects. I’ll try a bit harder to resist my usual antics this time. Let’s have a closer look at what the problem with my old pattern/rule based sentence-tokenization is.</p>
<h2 id="rules-vs-learning-from-context">Rules vs Learning from Context</h2>
<p>The problem is that punctuation is not enough - we need surrounding context! We will keep the example an English one for a wider-audience.</p>
<blockquote>
<p>GST Revenue Collection Figures stand at Rs.92,150 crore as on 23rd October, 2017; The total number of GSTR 3B returns filed for the month of September 2017 is 42.91 lakhs (as on 23.10.2017).</p>
</blockquote>
<p>Going by the usual sentence-delimiter ‘.’ alone in the above setting leads to ‘23.10.2017’ being treated as 3 different sentences with my currently implemented rule-based segmenter alone. The ambiguity here needs to be often resolved statistically using surrounding context. Same goes for ‘Rs.’. Now my problem is that 23 here would map to 23 in the Hindi sentence as well, which will seep through my checks in the pipeline. I have a gut-feeling that I am bound to get better translations and improved retrieval scores which I use to rank matching articles as well here.</p>
<p>One might think - languages like Hindi, Bengali etc. have a different delimiter. This should make the job easier in these languages. However, there are several documents on the web which use the period (full-stop) instead of the usual end-of-sentence-marker (<code>\u0965</code>), or the Devanagari <em>danda</em>. Go have a look at <a href="https://khabar.ndtv.com/">NDTV Khabar</a>, for example. It seems that humans have also managed to confuse readers by using the vertical-pipe instead (<code>|</code>).</p>
<p>As I start, I am already aware of some <a href="https://github.com/moses-smt/mosesdecoder/commit/103707002699a1e114a2f45c1ef1c2b20a981964">additions</a> by <a href="http://homepages.inf.ed.ac.uk/bhaddow">Barry Haddow</a>, which he possibly created while preparing the PMIndia Corpus. However the additions are in the moses and possibly the perl ecosystem, which I will shy-away because my ecosystem is built in python.</p>
<h2 id="enter-the-punkttokenizer">Enter the PunktTokenizer</h2>
<p>Punkt <span class="citation">(Kiss and Strunk, <a href="#ref-kiss2006unsupervised">2006</a>)</span> has existed for a while now, and it’s perhaps my lack of background in NLP why I wasn’t aware about the same. I am also curious as to why nobody tried to implement it for Indian Languages. I found an <a href="https://github.com/nltk/nltk_data/pull/144">attempt</a> which tried to create one for Malayalam, which didn’t get merged yet.</p>
<p>There is already an existing implementation in <a href="https://github.com/nltk/nltk/blob/develop/nltk/tokenize/punkt.py">NLTK</a> <span class="citation">(Bird et al., <a href="#ref-bird2009natural">2009</a>)</span>. So as I take on another project in the area, I thought, why not ensure that there are Punkt Tokenizers available publically for the community to use for Indian Languages. So I set-out to the self-assigned task in hand.</p>
<h2 id="hacking-punkttokenizer">Hacking PunktTokenizer</h2>
<p>As I begin, I am notoriously underestimating the time-required for this task. This is supposed to be a tiny part of what I am about to do. My hope is that I just need to get training code connected and it will out-of-the box work.</p>
<h3 id="getting-training-to-work">Getting training to work</h3>
<p><img src="https://imgs.xkcd.com/comics/dependency_2x.png" width="50%"></p>
<p><strong>Should be easy given a great idea and some existing NLTK implementation, correct?</strong></p>
<p>For a surprisingly robust idea and what should be widely adopted(?) idea, the tutorials on how to train and similar are not as abundantly available as I thought it would be. StackOverflow gave a couple of useful links (<a href="https://stackoverflow.com/questions/52150000/how-to-train-nltk-punktsentencetokenizer-batchwise">#1</a>, <a href="https://stackoverflow.com/questions/21160310/training-data-format-for-nltk-punkt">#2</a>). After one more level of digging, what I finally found worked to be repurposed for my use-case was at <a href="https://github.com/alvations/DLTK/blob/master/dltk/tokenize/tokenizer.py">alvations/dltk</a>. There’s a decent example in there, which I started repurposing for my needs.</p>
<p>Okay, I have managed to figure out the training routine. However, this doesn’t seem to be working for sentence-delimiters which are different, like Hindi’s <em>PurnaVirama</em> and Bengali’s whatever the delimiter is. Good thing is <a href="https://bnjasim.github.io/">Binu</a> has found a potential list of these and stored them into some <a href="https://github.com/jerinphilip/ilmulti/blob/ccdaf9f5ffdf06c921276092f19a62883fcaf8e0/ilmulti/segment/segmenters.py#L37">pattern-segmenter</a> in <a href="https://github.com/jerinphilip/ilmulti">ilmulti</a>. Some useful information exists in <a href="http://anoopk.in/">Anoop</a>’s <a href="https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/indicnlp/tokenize/sentence_tokenize.py">IndicNLPLibrary code</a> as well, where he does some accounting for numbers and abbreviations. But I have a policy of not doing anything for a single-language and hence me rooting for trained Punkt models.</p>
<h3 id="injecting-custom-delimiters">Injecting custom delimiters</h3>
<p>There’s an <a href="https://github.com/nltk/nltk/issues/2008">entire thread</a> by alvations again on some attempt - oh, I can start to empathize with the plight now. My thinking goes like this now, <a href="https://www.nltk.org/_modules/nltk/tokenize/punkt.html#PunktLanguageVars"><code>PunktLanguageVars</code></a> have to be customized per language inorder to be able to accomodate the custom delimiters in languages like Bengali, Hindi, Marathi, Urdu etc. Somewhere in the source I found this has to be overridden.</p>
<h4 id="process">Process</h4>
<p>The following are my reactions as I progress about getting this accomplished.</p>
<ol style="list-style-type: decimal">
<li>Ugghh, I actually need more knowledge of Punkt the paper and the implementation now.<br />
</li>
<li>Turns out not, I just modified the sentence-delimiters with some twisted python dynamic inheritance <a href="https://github.com/jerinphilip/ilmulti/blob/c589adfb1a23834041ac65deec35fe182ff2a92a/ilmulti/segment/punkt_segmenter/utils.py#L7-L30">workarounds</a> (might have been an overkill, if I look back). Marathi seems to be using full-stops.</li>
<li>Training might have improved with the additional stuff, I hope. But what about test? Found something on StackOverflow. <a href="https://stackoverflow.com/questions/29746635/nltk-sentence-tokenizer-custom-sentence-starters">nltk:custom-sentence-starters</a> The above doesn’t seem to be working, weird.<br />
</li>
<li>Seems like this is more effort than what it’s worth, thinking of lesser solutions that I can get away with. Decimal, Abbreviation ambiguity to be cleared in a first round, then use hard-delimiters for each language, like the Devanagiri <em>danda</em> in a second pass.</li>
<li>Finally managed a working solution after tinkering with the code for a while. I modified the first-pass-annotation function from punkt pulling the punkt implementation’s <a href="https://github.com/nltk/nltk/blob/b0cd83ded0c3c2394f878d8577d71187fa3f9ae4/nltk/tokenize/punkt.py">source</a>. second pass.</li>
</ol>
<h4 id="final-solutionworkaround">Final Solution/Workaround</h4>
<div class="sourceCode"><pre class="sourceCode diff"><code class="sourceCode diff"><span class="kw">diff --git a/sentence_tokenizers/punkt.py b/sentence_tokenizers/punkt.py</span>
index 408ce27..e30de2a 100644
<span class="dt">--- a/sentence_tokenizers/punkt.py</span>
+++ b/sentence_tokenizers/punkt.py
@@ -615,6 +615,10 @@ class PunktBaseClass(object):
                 aug_tok.abbr = True
             else:
                 aug_tok.sentbreak = True
+        else:
+            for sent_end_char in self._lang_vars.sent_end_chars:
+                if tok[-1] == sent_end_char:
+                    aug_tok.sentbreak = True
 
         return
 </code></pre></div>
<p>On a quick look, I can already tell that <code>tok[-1]</code> is a potential <code>IndexError</code> in the future at some point, maybe as I am not placing any guards. But this is not production code, we will handle it when an issue comes.</p>
<p>In the above ordering, you can observe me coasting through the points in below graph:</p>
<p><img src="https://i.redd.it/d0dxcnw57kb01.jpg" width="50%"></p>
<p>I would ideally wish to open a PR, communicate with the developers and merge the required things upstream in nltk, but for now I will find myself content with a working solution and move onward to immediately pressing things in hand.</p>
<p><strong>Edit 25 August 2020</strong>: The <a href="https://github.com/nltk/nltk/issues/2586">issue I opened</a> was addressed by one of the NLTK-devs who opened a <a href="https://github.com/nltk/nltk/pull/2587">PR</a>. I prefer the solution in the PR and have temporarily adopted it. I hope the changes eventually make upstream.</p>
<h2 id="merging-into-ilmulti">Merging into ilmulti</h2>
<p>Looks like I have some solution ready for sentence tokenization for Indian Languages. I was prototyping at <a href="https://github.com/jerinphilip/sentence-tokenizers">jerinphilip/sentence-tokenizers</a> I have the <a href="https://github.com/jerinphilip/ilmulti">ilmulti</a> repo prepared with some API which currently exists inside my head. Fitting the sentence-tokenizers I just built to the same provides ease of usage in my PIB pipeline.</p>
<p>This is what we will do next, and build the documentation along with the blog post in the process.</p>
<p>The API is rather simple:</p>
<div class="sourceCode"><pre class="sourceCode py"><code class="sourceCode python">
<span class="kw">class</span> BaseSegmenter:

    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):
        <span class="co"># Initialize with any required trained models, paths,</span>
        <span class="co"># language configurations.</span>

    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, content, lang<span class="op">=</span><span class="va">None</span>):
        <span class="co"># Find language if unspecified.</span>
        <span class="co"># Call the language specific sentence-tokenizer</span></code></pre></div>
<p>I usually have some lazy-load hack involved as well, and instances for a particular language are created only during the first call for the same and reused after.</p>
<p>I want to add some tests as well, at least the qualititative kind so people can quickly get started with the individual components. This time, I have managed to squeeze the tests, for a quick qualitative checks in two scripts (<a href="https://github.com/jerinphilip/ilmulti/blob/master/ilmulti/segment/punkt_segmenter/test.py">#1</a>, <a href="https://github.com/jerinphilip/ilmulti/blob/master/scripts/test_punkt.sh">#2</a>).</p>
<p>The final step is to check integration in the PIB crawl-environment that the sentence-tokenizers (which I call segmenters) are working as intended. At this stage, I can export a document NMT standard corpus with segment annotations from my raw-text so researchers can work in the area while applying principles or ideas to Indian Languages as well.</p>
<p><strong>Let the survivor bias kick in</strong>. This was cakewalk - took a few fixes to the code I wrote initially, but didn’t take much time getting there. Numbers decimal’s etc seem to be working nicely, I will still need to account for more abbreviations etc., which can eventually be improved as the data-situation improves, I hope..</p>
<h2 id="afterthoughts">Afterthoughts</h2>
<ol style="list-style-type: decimal">
<li>The current trained models of Punkt are not perhaps the best. But I believe I can eventually tap into the monolingual data in the likes of <a href="https://github.com/AI4Bharat/indicnlp_corpus">AI4Bharat</a> <span class="citation">(Kunchukuttan et al., <a href="#ref-kunchukuttan2020ai4bharat">2020</a>)</span>.</li>
<li>For a task among several other things done under 2 days, while not perfect, this is good enough a starting point. Maybe someone who follow-up the work in IIIT can take cleaning this up incrementally.</li>
<li>Once again, working my way around several stuff I have no clue how it runs under the hood - I have successfully managed to produce something of value. I intend to read up more on the likes of BleuAlign and Punkt later, but no time in hand now.</li>
<li>Using this in our pipeline mentioned in <span class="citation">Siripragada et al. (<a href="#ref-siripragada-etal-2020-multilingual">2020</a>)</span> and <span class="citation">Philip et al. (<a href="#ref-philip2020revisiting">2020</a>)</span> actually led to lesser sentences with more-articles (but I expect a consequent increment in mean-sentence-length or an eventual bugfix; Edit: eventual-bugfix is what happened.). Who knows, if the improved quality of the corpora might lead to better BLEU scores?</li>
<li>These should easily cover 11 languages which ilmulti operates on, but I won’t make many strong claims here.</li>
</ol>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-bird2009natural">
<p>Steven Bird, Ewan Klein, and Edward Loper. 2009. <em>Natural language processing with python: Analyzing text with the natural language toolkit</em>. “ O’Reilly Media, Inc.”, editions.</p>
</div>
<div id="ref-kiss2006unsupervised">
<p>Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual sentence boundary detection. <em>Computational linguistics</em>, 32(4):485–525.</p>
</div>
<div id="ref-kunchukuttan2020ai4bharat">
<p>Anoop Kunchukuttan, Divyanshu Kakwani, Satish Golla, Avik Bhattacharyya, Mitesh M Khapra, Pratyush Kumar, and others. 2020. AI4Bharat-indicnlp corpus: Monolingual corpora and word embeddings for indic languages. <em>arXiv preprint arXiv:2005.00085</em>.</p>
</div>
<div id="ref-philip2020revisiting">
<p>Jerin Philip, Shashank Siripragada, Vinay P Namboodiri, and CV Jawahar. 2020. Revisiting low resource status of indian languages in machine translation. <em>arXiv preprint arXiv:2008.04860</em>.</p>
</div>
<div id="ref-sennrich2011iterative">
<p>Rico Sennrich and Martin Volk. 2011. Iterative, mt-based sentence alignment of parallel texts. In <em>Proceedings of the 18th nordic conference of computational linguistics (nodalida 2011)</em>, pages 175–182.</p>
</div>
<div id="ref-siripragada-etal-2020-multilingual">
<p>Shashank Siripragada, Jerin Philip, Vinay P. Namboodiri, and C V Jawahar. 2020. A multilingual parallel corpora collection effort for Indian languages. In <em>Proceedings of the 12th language resources and evaluation conference</em>, pages 3743–3751, Marseille, France, May. European Language Resources Association.</p>
</div>
</div></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>
<entry>
    <title>TeX environment thoughts</title>
    <link href="http://jerinphilip.github.io/posts/latex-build-system.html" />
    <id>http://jerinphilip.github.io/posts/latex-build-system.html</id>
    <published>2020-05-10T00:00:00Z</published>
    <updated>2020-05-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:title" content="TeX environment thoughts" />
        <meta property="og:url" content="http://jerinphilip.github.io/posts/latex-build-system.html" />
        <title>TeX environment thoughts</title>
        <link rel="shortcut icon" href="../static/images/favicon.ico" type="image/x-icon">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,600,600i,700,700i,800" rel="stylesheet">
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css">
        <link rel="stylesheet" href="../static/css/tether.min.css" />
        <link rel="stylesheet" href="../static/css/bootstrap.min.css" />
        <link rel="stylesheet" href="../static/css/custom.css" />
        <link rel="stylesheet" href="../static/css/kate.css" />
        <link rel="stylesheet" href="../static/css/ekko-lightbox.css" />
        <script type="text/javascript" src="../static/js/jquery.min.js"></script>
        <script type="text/javascript" src="../static/js/tether.min.js"></script>
        <script type="text/javascript" src="../static/js/anchor.min.js"></script>
        <script type="text/javascript" src="../static/js/ekko-lightbox.min.js"></script>
        <script type="text/javascript" src="../static/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config" src="../static/js/mathjax-config.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML"></script>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122834696-1"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-122834696-1');
		</script>
    </head>
    <body>
            <div class="container">
                <div class="row">
                    <div class="col">
                        <header>
    <span><a href="../">/home/jerin</a> </span>
    <!--
    <span><a href="/archive.html">posts</a></span>,
    <span><a href="/contact.html">contact</a></span>
    -->
</header>

                    </div>
                </div>
                <div class="row">
                    <div class="col">
                        <h1>TeX environment thoughts</h1>
                        <div class="row mb-4">
    <div class="post-info col">
        <div>May 10, 2020</div>

         

         
            <div></div>
         
    </div>
</div>

<div class="row">
    <div class="col-md-8 col-sm-12"><h2 id="overleaf-vs-local-environment">Overleaf vs Local environment</h2>
<p>Overleaf has been very convenient to collaborate with other people while writing/reviewing scientific papers. I write scientific papers in Latex, which means the source can be treated like code, and versioning be enabled. I like git and the <a href="https://xkcd.com/1597/">superpowers</a> that come with it. In an ideal setting, I’d have the full git repository with branches and versioning with my collaborators. But it’s almost unlikely that I find such people and unnecessary overhead is added to the process. Luckily OverLeaf offers a neat bridge with git enabled versioning, which solves a lot of my problems.</p>
<p>So my typical paper-writing workflow is:</p>
<ol style="list-style-type: decimal">
<li>Start a project on OverLeaf,</li>
<li>do a git-pull to the local machine</li>
<li>Create a remote and push on a GitHub repository, because I like keeping reviews and comments as issues over there and track changes in source in the web UI.</li>
<li>Add a Makefile so local builds are easy and possible.</li>
<li>Use vim and evince to create an overleaf-y environment to edit locally.</li>
</ol>
<p>Keeping a few copies also lets you continue work in the unlikely event of an overleaf outage, or internet outage - which can happen.</p>
<h2 id="the-pros-and-cons">The Pros and Cons</h2>
<p>OverLeaf doesn’t allow full history access unless you are a premium user. Premium however is rather expensive, when I’m already skilled to use the versioning tools for software to track history. Not much of a plus, but I commit my changes, so I usually know which changes came from who.</p>
<p>Sometimes people tend to mess-up documents and be clueless what went wrong (like an unbalanced paranthesis somewhere etc), then spend a fair amount of timing fixing this. It’s usually very easy to check the diffs instead, and overleaf’s UI and history access isn’t particularly the most useful when it comes to all these. It also helps me to quickly know what changes my advisors have made, so I don’t retouch them to go around in circles.</p>
<p>I have been asked by a few why I don’t use something GUI like TexMaker. Perhaps I’m just too attached to vim. There are certain use-cases in vim, especially while working with latex-source or navigating the text fast which simply is very straightforward in vim. Here are a bunch of those:</p>
<ol style="list-style-type: decimal">
<li><a href="https://vim.fandom.com/wiki/Jumping_to_previously_visited_locations">jump-lists</a></li>
<li><a href="https://github.com/godlygeek/tabular">Tabular.vim</a></li>
<li>Indentation of math, etc which is possible in vim.</li>
<li>I can <code>\include{...}</code> files and use <code>C-w-g-f</code> (Control-window-go-to-file) to open the file quickly in a new tab.</li>
</ol>
<p>Moving around files is especially useful, when the nature of your document is more book-like (a thesis, which I’m trying to wrap up soon) and the flat structure of the usual paper-template is not perhaps what you begin and proceed with.</p>
<p>Not having <a href="https://www.tug.org/TUGboat/tb29-3/tb93laurens.pdf">SyncTex</a> is kind of a bummer, but not really a problem to me, as it has failed several times for me and is not something that I would like to get used to.</p>
<h2 id="windows-adaptation">Windows adaptation</h2>
<p>The above setting is very straightforward while in a Linux ecosystem. Previously I was a heavy user of Linux distributions (Ubuntu, ArchLinux), which made all the trickery convenient. But this year I decided to make my life simpler about the time I got a new laptop, a ThinkPad X1 Carbon (Generation 7). It came pre-loaded with Windows 10 and I had heard good reviews from Aditya about the Windows Subsystem for Linux (WSL). WSL2 gave me decent start on Windows (I’d totally recommend it, I’ve been trying advanced things with it and it works decent enough).</p>
<p>I don’t need to say much here, because once you have WSL2 it’s almost the same as how you would do this on Linux.</p>
<p>Adobe Reader opened the PDF to edit (not read-only), which made rebuilding the PDF throw errors. I solved the first issue by installing evince from <a href="https://www.fosshub.com/Evince.html">here</a>. Opens read-only and is good enough to work with. (Also, windows apps can be neatly launched from bash-shell inside WSL. I simply added a symlink to the <code>.exe</code> which lauches evince.</p>
<h2 id="latexmk">LatexMk</h2>
<p>I discovered Latexmk (I know, bit late) and had to rework the usual <code>Makefile</code> to enable the same. Below is the Makefile using latexmk that I use.</p>
<script src="https://gist.github.com/jerinphilip/c9360753ba498546df707b23fdfe3207.js"></script>
<p>I lifted somewhere from the internet, the author I can’t seem to track now. There is a modification to build to a directory, with <code>JOBNAME</code>.</p>
<p>To use evince as the default pdf viewing tool, you might possibly want to edit your <code>~/.latexmkrc</code> with the following line:</p>
<div class="sourceCode"><pre class="sourceCode php"><code class="sourceCode php"><span class="kw">$pdf_previewer</span> = <span class="st">'evince'</span><span class="ot">;</span></code></pre></div>
<h2 id="what-to-tinker-with-next">What to tinker with next?</h2>
<p><strong>WSL</strong>, particularly WSL2 is very exciting for me. I have been trying to play around with <a href="https://pidgin.im/">pidgin</a> source code in my free time, hoping I can do some plugin hacking in the future. Building and modifying seems very convenient with the WSL system (unlike my past experiences with Git Bash / Cygwin / etc).</p>
<p><strong>LaTeX builds</strong> I have been using XeLaTeX recently because I’ve had to work with Indian language scripts and the fonts, which are not natively supported by pdflatex. While trying to solve a certain problem, I have found hints that LuaLatex is perhaps a better choice in terms of build speed etc for diagrams. Maybe, when the need becomes critical or there is some free time.</p></div>
</div>


<div class="row">
    <div class="col">
        <p class="text-muted font-italic"> (Comments disabled. Email me instead.) </p>
    </div>
</div>

                    </div>
                </div>

                <div class="row">
                    <div class="col">
                        
<footer>
    <small>Site generated using <a href="http://jaspervdj.be/hakyll">Hakyll</a></small>
</footer>

                    </div>
                </div>
            </div>
        <script type="text/javascript" src="../static/js/init.js"></script>
    </body>
</html>
]]></summary>
</entry>

</feed>
